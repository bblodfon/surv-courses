[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Material on Survival Analyses",
    "section": "",
    "text": "Intro\nThe analyses you see here come from various courses related to survival analysis, epidemiology and causal inference:\n\nModern methods for analyzing survival and time to event data (2022) link\nCausal inference for time-to-event outcomes (2024) link",
    "crumbs": [
      "Intro"
    ]
  },
  {
    "objectID": "na_km_logrank.html",
    "href": "na_km_logrank.html",
    "title": "Non-parametric estimators",
    "section": "",
    "text": "Nelson-Aalen estimator\nlibrary(survival)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nIn the period 1962-77 a total of \\(205\\) patients with malignant melanoma (cancer of the skin) were operated at Odense University hospital in Denmark. A number of covariates were recorded at operation, and the patients were followed up until death or censoring at the end of the study at December 31, 1977. We will study death from malignant melanoma considering death from other causes as censorings. (Source: Andersen, Borgan, Gill & Keiding, Springer, 1993.)\nYou may read the data into R by the command:\nmelanoma = read.table(\"https://www.med.uio.no/imb/english/research/centres/ocbe/courses/imb9335/melanoma.txt\",header=T) %&gt;% \n  as_tibble()\nThe data are organized with one line for each of the 205 patients, and with the following variables in the seven columns:\nThe following commands provide a Nelson-Aalen plot for female melanoma patients. (The survival-library has to be loaded.)\nsurv.f = survival::survfit(Surv(lifetime,status==1) ~ 1, data = melanoma, \n  subset = (sex == 1), ctype = 1) # ctype = 1 means NA estimator\nsurv.m = survival::survfit(Surv(lifetime,status==1) ~ 1, data = melanoma, \n  subset = (sex == 2), ctype = 1)\n\nplot(surv.f, fun=\"cumhaz\", mark.time=FALSE, conf.type=\"plain\", xlim=c(0,10), ylim=c(0,0.80), main=\"Females\", xlab=\"Years since operation\", ylab=\"Cumulative hazard\")\n\n\n\n\n\n\n\nplot(surv.m, fun=\"cumhaz\", mark.time=FALSE, conf.type=\"plain\", xlim=c(0,10), ylim=c(0,0.80), main=\"Males\", xlab=\"Years since operation\", ylab=\"Cumulative hazard\")\nTo plot the Nelson-Aalen estimates for both genders in the same figure, we may give the commands:\nsurv.sex = survival::survfit(Surv(lifetime,status==1) ~ strata(sex), data=melanoma, ctype=1)\n\nplot(surv.sex, fun=\"cumhaz\", mark.time=FALSE, lty=1:2, xlim=c(0,10), ylim=c(0,0.80), main=\"Gender\", xlab=\"Years since operation\", ylab=\"Cumulative hazard\")\n\nlegend(\"topleft\", c(\"Females\",\"Males\"), lty=1:2)\nsurv.ulcer = survival::survfit(Surv(lifetime,status==1) ~ strata(ulcer), data=melanoma, ctype=1)\n\nplot(surv.ulcer, fun=\"cumhaz\", mark.time=FALSE, lty=1:2, xlim=c(0,10), ylim=c(0,0.80), main=\"Ulceration\", xlab=\"Years since operation\", ylab=\"Cumulative hazard\")\n\nlegend(\"topleft\", c(\"Present\",\"Absent\"), lty=1:2)\nsurv.grthick = survival::survfit(Surv(lifetime,status==1) ~ strata(grthick), data=melanoma, ctype=1)\n\nplot(surv.grthick, fun=\"cumhaz\", mark.time=FALSE, lty=1:3, xlim=c(0,10), \n  ylim=c(0,0.80), main=\"Tumor thickness\", xlab=\"Years since operation\", ylab=\"Cumulative hazard\")\n\nlegend(\"topleft\", c(\"0-1 mm\",\"2-4 mm\", \"5+ mm\"), lty=1:3)",
    "crumbs": [
      "Non-parametric estimators"
    ]
  },
  {
    "objectID": "na_km_logrank.html#nelson-aalen-estimator",
    "href": "na_km_logrank.html#nelson-aalen-estimator",
    "title": "Non-parametric estimators",
    "section": "",
    "text": "status: status (1=death from disease, 2=censored, 4=death from other cause)\nlifetime: life time from operation in years\nulcer: ulceration (1=present, 2=absent)\nthickn: tumor thickness in mm\nsex: 1=female, 2=male\nage: age at operation in years\ngrthick: grouped tumor thickness [1: 0-1 mm (i.e. below 2 mm), 2: 2-4 mm (i.e. at least 2 mm, but below 5 mm) , 3: 5+ mm (i.e. 5 mm or more)]\n\n\n\n\nPerform the commands and interpret the Nelson-Aalen plot.\nMake a Nelson-Aalen plot for males and compare with the plot for females.\n\n\n\n\nPerform the commands and inspect the plot.\n\n\nStraight slope =&gt; constant hazard independent of gender\n\n\nMake Nelson-Aalen plots for patients with ulceration present and absent and interpret the plots. (Ulceration is “present” if the surface of the tumor viewed in a microscope show signs of ulcers and “absent” otherwise.)\n\n\n\nMake Nelson-Aalen plots for the three thickness groups 0-1 mm, 2-4 mm, 5+ mm and interpret the plots.",
    "crumbs": [
      "Non-parametric estimators"
    ]
  },
  {
    "objectID": "na_km_logrank.html#kaplan-meier-and-log-rank-test",
    "href": "na_km_logrank.html#kaplan-meier-and-log-rank-test",
    "title": "Non-parametric estimators",
    "section": "Kaplan-Meier and log-rank test",
    "text": "Kaplan-Meier and log-rank test\nIn this exercise, we will use the Kaplan-Meier estimator and the log-rank test to study survival for the melanoma patients.\nWe will consider Kaplan-Meier estimates for the mortality from malignant melanoma treating death from other causes as censoring.\nWe may compute and plot the Kaplan-Meier estimate of the survival distribution for male patients by the commands (you need to load the survival-library)\n\nfit.m=survfit(Surv(lifetime,status==1)~1,data=melanoma, subset=(sex==2), conf.type=\"plain\")\nplot(fit.m, mark.time=FALSE, xlab=\"Years after operation\", main = \"Males\")\nabline(h = 0.75, col=\"red\")\nabline(h = 0.5, col=\"blue\")\n\n\n\n\n\n\n\n\nTo obtain a summary of the results, you may give the command:\n\n#?quantile.survfit\nsummary(fit.m)\n\nCall: survfit(formula = Surv(lifetime, status == 1) ~ 1, data = melanoma, \n    subset = (sex == 2), conf.type = \"plain\")\n\n  time n.risk n.event survival std.err lower 95% CI upper 95% CI\n 0.507     76       1    0.987  0.0131        0.961        1.000\n 0.559     75       1    0.974  0.0184        0.938        1.000\n 0.575     74       1    0.961  0.0223        0.917        1.000\n 0.636     73       1    0.947  0.0256        0.897        0.998\n 1.167     72       1    0.934  0.0284        0.878        0.990\n 1.449     70       1    0.921  0.0310        0.860        0.982\n 1.701     69       1    0.908  0.0333        0.842        0.973\n 1.723     68       1    0.894  0.0354        0.825        0.964\n 1.805     67       1    0.881  0.0373        0.808        0.954\n 1.967     66       1    0.867  0.0390        0.791        0.944\n 2.060     65       1    0.854  0.0407        0.774        0.934\n 2.134     64       1    0.841  0.0422        0.758        0.923\n 2.173     63       1    0.827  0.0435        0.742        0.913\n 2.649     62       1    0.814  0.0448        0.726        0.902\n 2.677     61       1    0.801  0.0461        0.710        0.891\n 2.852     60       1    0.787  0.0472        0.695        0.880\n 2.910     59       1    0.774  0.0482        0.680        0.869\n 2.945     58       1    0.761  0.0492        0.664        0.857\n 3.364     57       1    0.747  0.0501        0.649        0.846\n 3.932     55       1    0.734  0.0510        0.634        0.834\n 4.126     53       1    0.720  0.0519        0.618        0.822\n 4.153     51       1    0.706  0.0528        0.602        0.809\n 4.340     50       1    0.692  0.0536        0.587        0.797\n 4.630     47       1    0.677  0.0544        0.570        0.784\n 5.647     32       1    0.656  0.0567        0.545        0.767\n 5.762     29       1    0.633  0.0591        0.517        0.749\n 6.542     25       1    0.608  0.0619        0.487        0.729\n 7.027     22       1    0.580  0.0650        0.453        0.708\n 7.622     21       1    0.553  0.0675        0.420        0.685\n\n\n\nPerform these commands and interpret the Kaplan-Meier plot. Determine the lower quartile of the survival distribution for males with 95% confidence limits using the output from the summary-command. (Note that the lower quartile corresponds to 75% survival probability.)\n\n\nLook at the first time the Survival probability falls below \\(75\\%\\)\n\n\nWe may obtain the quartiles of the survival distribution for males by the command\n\n\nquantile(fit.m)\n\n$quantile\n      25       50       75 \n3.364384       NA       NA \n\n$lower\n      25       50       75 \n2.172603 6.542466       NA \n\n$upper\n      25       50       75 \n5.761644       NA       NA \n\n\nPerform this command and compare with the result you obtained in a).\n\nMake a Kaplan-Meier plot for females, and determine the lower quartile for females with 95% confidence limits (if possible). Compare with the results for males.\n\n\nfit.f=survfit(Surv(lifetime,status==1)~1,data=melanoma, subset=(sex==1), conf.type=\"plain\")\nplot(fit.f, mark.time=FALSE, xlab=\"Years after operation\", main = \"Females\")\n\n\n\n\n\n\n\nquantile(fit.f)\n\n$quantile\n      25       50       75 \n8.334247       NA       NA \n\n$lower\n     25      50      75 \n5.29589      NA      NA \n\n$upper\n25 50 75 \nNA NA NA \n\n\n\nUse the log-rank test to test the null hypothesis that males and females have the same mortality from malignant melanoma:\n\n\nsurvdiff(Surv(lifetime,status==1) ~ sex,data=melanoma)\n\nCall:\nsurvdiff(formula = Surv(lifetime, status == 1) ~ sex, data = melanoma)\n\n        N Observed Expected (O-E)^2/E (O-E)^2/V\nsex=1 126       28     37.1      2.25      6.47\nsex=2  79       29     19.9      4.21      6.47\n\n Chisq= 6.5  on 1 degrees of freedom, p= 0.01 \n\n\nWhat may you conclude from the test?\n\nMake Kaplan-Meier plots for patients with ulceration present and absent and interpret the results. Is it possible to estimate the lower quartile for both ulceration groups? Estimate the lower quartile with confidence limits if possible. Is there a significant difference in cancer mortality for patients with ulceration present and absent?\n\n\nplot(surv.ulcer, mark.time=FALSE, lty = 1:2, xlab=\"Years after operation\", main = \"Ulceration\")\nlegend(\"bottomleft\", c(\"Present\",\"Absent\"), lty=1:2)\n\n\n\n\n\n\n\n\n\nquantile(surv.ulcer)\n\n$quantile\n                            25       50 75\nstrata(ulcer)=ulcer=1 2.690411 8.334247 NA\nstrata(ulcer)=ulcer=2       NA       NA NA\n\n$lower\n                            25       50 75\nstrata(ulcer)=ulcer=1 2.060274 4.728767 NA\nstrata(ulcer)=ulcer=2 7.621918       NA NA\n\n$upper\n                            25 50 75\nstrata(ulcer)=ulcer=1 4.126027 NA NA\nstrata(ulcer)=ulcer=2       NA NA NA\n\n\n\n# NO STRATA IN THE FORMULA NEEDED!\nsurvdiff(formula = Surv(lifetime, status == 1) ~ ulcer, data = melanoma)\n\nCall:\nsurvdiff(formula = Surv(lifetime, status == 1) ~ ulcer, data = melanoma)\n\n          N Observed Expected (O-E)^2/E (O-E)^2/V\nulcer=1  90       41     21.2      18.5      29.6\nulcer=2 115       16     35.8      10.9      29.6\n\n Chisq= 29.6  on 1 degrees of freedom, p= 5e-08 \n\n\n\nMake Kaplan-Meier plots for the three thickness groups 0-1 mm, 2-4 mm, 5+ mm and interpret the plots. Estimate the lower quartile with confidence limits if possible. Is there a significant difference in cancer mortality between the three thickness groups?\n\n\nSame procedure as above!!!",
    "crumbs": [
      "Non-parametric estimators"
    ]
  },
  {
    "objectID": "cox_regr.html",
    "href": "cox_regr.html",
    "title": "Cox Regression (low-dim)",
    "section": "",
    "text": "library(survival)\nlibrary(tibble)\n\nFrom 1962 to 1969 a number of patients with liver cirrhosis at several hospitals in Copenhagen were included in a randomized clinical trial. The purpose of the study was to investigate whether patients treated with the hormone prednisone had a better survival than patients who got an inactive placebo treatment. In this exercise, we will restrict attention to the \\(386\\) patients in the study who had no excess fluid in the abdomen at entry. (Source: Andersen, Borgan, Gill & Keiding, Springer, 1993.)\nWe will study the effect of treatment with prednisone. We will also investigate the effect on survival of the covariates sex, age, and prothrombin index (a measurement based on a blood test of some coagulation factors produced by the liver, in percent of the normal value).\nYou may read the data into R by the command:\n\nliver = read.table(\"https://www.med.uio.no/imb/english/research/centres/ocbe/courses/imb9335/liver.txt\", header=T) %&gt;% as_tibble()\nliver\n\n# A tibble: 386 × 6\n   status  time treat   sex   age  prot\n    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1      0    22     0     0    75    94\n 2      0    24     0     0    53    26\n 3      0    24     1     0    76    43\n 4      0    25     0     1    46   100\n 5      1    27     0     1    76    71\n 6      0    29     0     1    51    68\n 7      0    30     1     1    49    72\n 8      0    30     0     1    36    84\n 9      0    30     1     1    54    60\n10      1    33     1     0    60   100\n# ℹ 376 more rows\n\n\nThe data are organized with one line for each of the \\(386\\) patients, and with the following variables in the six columns:\n\nstatus: indicator for death/censoring (1=dead; 0=censored)\ntime: time in days from start of treatment to death/censoring\ntreat: treatment (0=prednisone; 1=placebo)\nsex: gender (0=female; 1=male)\nage: age in years at start of treatment\nprot: prothrombin index\n\nTo get an overview of the data, we will first do univariate analyses of the effects of the covariates treatment, sex, age and prothrombin index using Kaplan-Meier plots and the log-rank tests.\nFor treatment, you may give the commands (when time is converted to years).\n\nfit.treat = survfit(Surv(time/365.25, status) ~ factor(treat), data = liver)\n\nplot(fit.treat, mark.time=FALSE, xlab=\"Years after randomization\", lty=1:2, main=\"KM - Treatment\")\nlegend(\"bottomleft\",c(\"Prednisone\",\"Placebo\"),lty=1:2)\n\n\n\n\n\n\n\n\n\nsurvdiff(Surv(time/365.25,status) ~ factor(treat), data=liver)\n\nCall:\nsurvdiff(formula = Surv(time/365.25, status) ~ factor(treat), \n    data = liver)\n\n                  N Observed Expected (O-E)^2/E (O-E)^2/V\nfactor(treat)=0 191       94      111      2.56      5.42\nfactor(treat)=1 195      117      100      2.83      5.42\n\n Chisq= 5.4  on 1 degrees of freedom, p= 0.02 \n\n\n\nPerform the commands and interpret the results.\nPerform similar analyses as in a) for the covariate sex.\n\n\nfit.sex = survfit(Surv(time/365.25, status) ~ factor(sex), data = liver)\n\nplot(fit.sex, mark.time=FALSE, xlab=\"Years after randomization\", lty=1:2, main=\"KM - Sex\")\nlegend(\"bottomleft\", c(\"Female\",\"Male\"), lty=1:2)\n\n\n\n\n\n\n\n\n\nsurvdiff(formula = Surv(time/365.25, status) ~ factor(sex), data = liver)\n\nCall:\nsurvdiff(formula = Surv(time/365.25, status) ~ factor(sex), data = liver)\n\n                N Observed Expected (O-E)^2/E (O-E)^2/V\nfactor(sex)=0 159       81     91.8     1.268      2.25\nfactor(sex)=1 227      130    119.2     0.977      2.25\n\n Chisq= 2.3  on 1 degrees of freedom, p= 0.1 \n\n\nWe want to do similar analyses as in a) and b) for the numeric covariates age and prothrombin index. But then we first have to make a grouping based on these covariates. In order to group age into the groups: 49 years or less, 50-59 years, 60-69 years, and 70 years or more, you may create a new categorial covariate agegroup by the command:\n\nliver$agegroup=cut(liver$age, breaks=c(0,49,59,69,100),labels=1:4)\nunique(liver$agegroup)\n\n[1] 4 2 1 3\nLevels: 1 2 3 4\n\n\n\nMake a grouping of age by the command given above, and perform similar analyses for age group as the ones in a) and b).\n\n\nfit.ageg = survfit(Surv(time/365.25, status) ~ factor(agegroup), data = liver)\n\nplot(fit.ageg, mark.time=FALSE, xlab=\"Years after randomization\", lty=1:4, main=\"KM - Age group\", col = 1:4)\nlegend(\"bottomleft\", c(\"0-49\",\"50-59\", \"60-69\", \"70-100\"), lty=1:4, col = 1:4)\n\n\n\n\n\n\n\n\n\n# factor() doesn't play a role here =&gt; in Cox regression it does!\nsurvdiff(formula = Surv(time/365.25, status) ~ factor(agegroup), data = liver)\n\nCall:\nsurvdiff(formula = Surv(time/365.25, status) ~ factor(agegroup), \n    data = liver)\n\n                     N Observed Expected (O-E)^2/E (O-E)^2/V\nfactor(agegroup)=1  74       23     46.4     11.78     15.16\nfactor(agegroup)=2 116       59     69.4      1.55      2.33\nfactor(agegroup)=3 146       97     73.2      7.78     12.05\nfactor(agegroup)=4  50       32     22.1      4.44      4.98\n\n Chisq= 25.8  on 3 degrees of freedom, p= 1e-05 \n\n\n\nMake an appropriate grouping of prothrombin index and do similar analyses for grouped prothrombin as in a), b) and c).\n\n\nquantile(liver$prot)\n\n  0%  25%  50%  75% 100% \n  26   58   72   90  135 \n\nsummary(liver$prot)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  26.00   58.00   72.00   73.55   90.00  135.00 \n\nliver$protgroup = cut(liver$prot, breaks=c(0,58,90,135),labels=1:3)\nunique(liver$protgroup)\n\n[1] 3 1 2\nLevels: 1 2 3\n\n\n\nfit.protg = survfit(Surv(time/365.25, status) ~ factor(protgroup), data = liver)\n\nplot(fit.protg, mark.time=FALSE, xlab=\"Years after randomization\", lty=1:3, main=\"KM - Protgroup\", col = 1:3)\nlegend(\"bottomleft\", c(\"0-58\",\"59-90\", \"91-135\"), lty=1:3, col = 1:3)\n\n\n\n\n\n\n\n\n\nHigher prot-index is better!!!\n\n\nsurvdiff(formula = Surv(time/365.25, status) ~ factor(protgroup), data = liver)\n\nCall:\nsurvdiff(formula = Surv(time/365.25, status) ~ factor(protgroup), \n    data = liver)\n\n                      N Observed Expected (O-E)^2/E (O-E)^2/V\nfactor(protgroup)=1 104       71     52.8     6.285     8.407\nfactor(protgroup)=2 190       99    102.8     0.137     0.268\nfactor(protgroup)=3  92       41     55.5     3.769     5.127\n\n Chisq= 10.2  on 2 degrees of freedom, p= 0.006 \n\n\nIn order to study jointly the effects the covariates, we fit a Cox regression model with all covariates. If we use age and prothrombin as numeric covariates, we may give the commands:\n\ncox.fit = coxph(Surv(time/365.25, status) ~ factor(treat) + factor(sex) + age + prot, data = liver)\n\nsummary(cox.fit)\n\nCall:\ncoxph(formula = Surv(time/365.25, status) ~ factor(treat) + factor(sex) + \n    age + prot, data = liver)\n\n  n= 386, number of events= 211 \n\n                    coef exp(coef)  se(coef)      z Pr(&gt;|z|)    \nfactor(treat)1  0.292844  1.340233  0.139504  2.099  0.03580 *  \nfactor(sex)1    0.382945  1.466597  0.145403  2.634  0.00845 ** \nage             0.046925  1.048043  0.008038  5.838 5.28e-09 ***\nprot           -0.010607  0.989449  0.003351 -3.165  0.00155 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n               exp(coef) exp(-coef) lower .95 upper .95\nfactor(treat)1    1.3402     0.7461     1.020     1.762\nfactor(sex)1      1.4666     0.6819     1.103     1.950\nage               1.0480     0.9542     1.032     1.065\nprot              0.9894     1.0107     0.983     0.996\n\nConcordance= 0.65  (se = 0.021 )\nLikelihood ratio test= 53.44  on 4 df,   p=7e-11\nWald test            = 50.36  on 4 df,   p=3e-10\nScore (logrank) test = 50.44  on 4 df,   p=3e-10\n\n\n\nTreat = 1 (placebo) or sex = 1 (male) or higher age is worse (HR = exp(coef) &gt; 1) given that all the other covariates remain the same\nAlso smaller prot is worse (but not too much, also the same for age)\n\n\nPerform the commands, and interpret the estimated hazard ratios.\nDo you think that it is reasonable to use both age and prothrombin index as numeric covariates in the Cox regression model? If not, also fit model(s) where you use the grouped version of one or both of these covariates. Interpret the estimated hazard ratios.\n\n\ncox.fit2 = coxph(Surv(time/365.25, status) ~ factor(treat) + factor(sex) + agegroup + protgroup, data = liver)\nsummary(cox.fit2)\n\nCall:\ncoxph(formula = Surv(time/365.25, status) ~ factor(treat) + factor(sex) + \n    agegroup + protgroup, data = liver)\n\n  n= 386, number of events= 211 \n\n                  coef exp(coef) se(coef)      z Pr(&gt;|z|)    \nfactor(treat)1  0.3267    1.3863   0.1406  2.324 0.020151 *  \nfactor(sex)1    0.3211    1.3787   0.1443  2.225 0.026057 *  \nagegroup2       0.4852    1.6245   0.2468  1.966 0.049307 *  \nagegroup3       1.0176    2.7665   0.2333  4.362 1.29e-05 ***\nagegroup4       1.0624    2.8933   0.2758  3.852 0.000117 ***\nprotgroup2     -0.3518    0.7034   0.1583 -2.223 0.026249 *  \nprotgroup3     -0.5966    0.5507   0.1972 -3.025 0.002489 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n               exp(coef) exp(-coef) lower .95 upper .95\nfactor(treat)1    1.3863     0.7213    1.0524    1.8261\nfactor(sex)1      1.3787     0.7253    1.0390    1.8294\nagegroup2         1.6245     0.6156    1.0015    2.6351\nagegroup3         2.7665     0.3615    1.7514    4.3701\nagegroup4         2.8933     0.3456    1.6851    4.9679\nprotgroup2        0.7034     1.4216    0.5158    0.9593\nprotgroup3        0.5507     1.8160    0.3741    0.8106\n\nConcordance= 0.645  (se = 0.02 )\nLikelihood ratio test= 46.95  on 7 df,   p=6e-08\nWald test            = 43.47  on 7 df,   p=3e-07\nScore (logrank) test = 44.97  on 7 df,   p=1e-07\n\n\nThis exercise is a continuation of the exercise on Cox regression for the liver cirrhosis data from yesterday. The liver cirrhosis data are described in the exercise from yesterday. There it is also explained how you may read the data into R.\nWe first fit a Cox regression model with all four covariates by the commands:\n\ncox.fit=coxph(Surv(time/365.25,status)~factor(treat)+factor(sex)+age+prot,data=liver)\nsummary(cox.fit)\n\nCall:\ncoxph(formula = Surv(time/365.25, status) ~ factor(treat) + factor(sex) + \n    age + prot, data = liver)\n\n  n= 386, number of events= 211 \n\n                    coef exp(coef)  se(coef)      z Pr(&gt;|z|)    \nfactor(treat)1  0.292844  1.340233  0.139504  2.099  0.03580 *  \nfactor(sex)1    0.382945  1.466597  0.145403  2.634  0.00845 ** \nage             0.046925  1.048043  0.008038  5.838 5.28e-09 ***\nprot           -0.010607  0.989449  0.003351 -3.165  0.00155 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n               exp(coef) exp(-coef) lower .95 upper .95\nfactor(treat)1    1.3402     0.7461     1.020     1.762\nfactor(sex)1      1.4666     0.6819     1.103     1.950\nage               1.0480     0.9542     1.032     1.065\nprot              0.9894     1.0107     0.983     0.996\n\nConcordance= 0.65  (se = 0.021 )\nLikelihood ratio test= 53.44  on 4 df,   p=7e-11\nWald test            = 50.36  on 4 df,   p=3e-10\nScore (logrank) test = 50.44  on 4 df,   p=3e-10\n\n\n\nDiscuss the assumptions of the model fitted above.\n\nCHECK CONTINUOUS VARIABLES LOG-LINEARITY ASSUMPTION:\n\nInvestigate if there is a log-linear effect of age, we may give the commands:\n\n\ncox.spage=coxph(Surv(time/365.25,status)~factor(treat)+factor(sex)+pspline(age)+prot,data=liver)\nprint(cox.spage)\n\nCall:\ncoxph(formula = Surv(time/365.25, status) ~ factor(treat) + factor(sex) + \n    pspline(age) + prot, data = liver)\n\n                         coef se(coef)      se2    Chisq   DF       p\nfactor(treat)1        0.29335  0.14085  0.14049  4.33754 1.00  0.0373\nfactor(sex)1          0.37413  0.14599  0.14595  6.56715 1.00  0.0104\npspline(age), linear  0.04689  0.00820  0.00820 32.71358 1.00 1.1e-08\npspline(age), nonlin                             1.66153 3.09  0.6609\nprot                 -0.01081  0.00337  0.00337 10.26994 1.00  0.0014\n\nIterations: 4 outer, 12 Newton-Raphson\n     Theta= 0.802 \nDegrees of freedom for terms= 1.0 1.0 4.1 1.0 \nLikelihood ratio test=56.1  on 7.08 df, p=1e-09\nn= 386, number of events= 211 \n\ntermplot(cox.spage,se=T,terms=3)\n\n\n\n\n\n\n\n\nDiscuss if it is reasonable to assume a log-linear effect of age? =&gt; No =&gt; plot is linear\n\nInvestigate if there is a log-linear effect of prothrombin index.\n\n\ncox.spprot=coxph(Surv(time/365.25,status)~factor(treat)+factor(sex)+age+pspline(prot),data=liver)\nprint(cox.spprot)\n\nCall:\ncoxph(formula = Surv(time/365.25, status) ~ factor(treat) + factor(sex) + \n    age + pspline(prot), data = liver)\n\n                          coef se(coef)      se2    Chisq   DF       p\nfactor(treat)1         0.31796  0.14161  0.14126  5.04114 1.00  0.0248\nfactor(sex)1           0.41439  0.14879  0.14846  7.75624 1.00  0.0054\nage                    0.04749  0.00811  0.00808 34.33445 1.00 4.6e-09\npspline(prot), linear -0.01031  0.00317  0.00317 10.60438 1.00  0.0011\npspline(prot), nonlin                             3.66752 3.05  0.3077\n\nIterations: 5 outer, 14 Newton-Raphson\n     Theta= 0.855 \nDegrees of freedom for terms= 1.0 1.0 1.0 4.1 \nLikelihood ratio test=57.7  on 7.04 df, p=5e-10\nn= 386, number of events= 211 \n\ntermplot(cox.spprot,se=T,terms=4)\n\n\n\n\n\n\n\n\nThe result above gives some indication (but not very clear!) that the effect of prothrombin index is not log-linear. In the remainder of this exercise, we will work with prothrombin grouped in the three groups: (i) prothrombin index 49 or below, (ii) prothrombin index 50-89, and (iii) prothrombin index 90 or above. We may create a new categorial covariate protgroup by the command:\n\nliver$protgroup=cut(liver$prot, breaks=c(0,49,89,150),labels=1:3)\n\nWe may fit a Cox model with this variable (together with the other three covariates) by the commands\n\ncox.grfit=coxph(Surv(time/365.25,status)~factor(treat)+factor(sex)+age+factor(protgroup),data=liver)\nsummary(cox.grfit)\n\nCall:\ncoxph(formula = Surv(time/365.25, status) ~ factor(treat) + factor(sex) + \n    age + factor(protgroup), data = liver)\n\n  n= 386, number of events= 211 \n\n                        coef exp(coef)  se(coef)      z Pr(&gt;|z|)    \nfactor(treat)1      0.289942  1.336351  0.139747  2.075 0.038008 *  \nfactor(sex)1        0.413701  1.512405  0.147978  2.796 0.005179 ** \nage                 0.047641  1.048794  0.008003  5.953 2.63e-09 ***\nfactor(protgroup)2 -0.578836  0.560550  0.182306 -3.175 0.001498 ** \nfactor(protgroup)3 -0.781223  0.457846  0.211930 -3.686 0.000228 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                   exp(coef) exp(-coef) lower .95 upper .95\nfactor(treat)1        1.3364     0.7483    1.0162    1.7574\nfactor(sex)1          1.5124     0.6612    1.1316    2.0213\nage                   1.0488     0.9535    1.0325    1.0654\nfactor(protgroup)2    0.5606     1.7840    0.3921    0.8013\nfactor(protgroup)3    0.4578     2.1841    0.3022    0.6936\n\nConcordance= 0.654  (se = 0.021 )\nLikelihood ratio test= 56.69  on 5 df,   p=6e-11\nWald test            = 55.11  on 5 df,   p=1e-10\nScore (logrank) test = 55.75  on 5 df,   p=9e-11\n\n\nCheck if the assumption of proportional hazards seems to be fulfilled for the model: a non-significant p-value means PH is okay/satisfied, since \\(H_0 = (slope = 0)\\), i.e. PH means that the time-dependent plot should approximately be a straight line:\n\nphres = cox.zph(cox.grfit, terms = FALSE)\nphres\n\n                   chisq df    p\nfactor(treat)1     0.892  1 0.34\nfactor(sex)1       1.653  1 0.20\nage                0.264  1 0.61\nfactor(protgroup)2 1.372  1 0.24\nfactor(protgroup)3 0.228  1 0.63\nGLOBAL             3.836  5 0.57\n\nplot(phres)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck this dependence with time: \\(b_1\\) \\(b_1+b_{12}\\times log(t)\\)\n\nphres = cox.zph(cox.grfit, transform = 'log', terms = FALSE)\nphres\n\n                   chisq df    p\nfactor(treat)1     0.240  1 0.62\nfactor(sex)1       1.006  1 0.32\nage                0.593  1 0.44\nfactor(protgroup)2 1.406  1 0.24\nfactor(protgroup)3 0.660  1 0.42\nGLOBAL             2.738  5 0.74\n\nplot(phres)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting survival curves for new patients:\n\nA prednisone treated woman aged 60 years who has prothrombin index 90 or above\nA prednisone treated woman aged 60 years who has prothrombin index 49 or below\nA placebo treated man aged 60 years who has prothrombin index 90 or above\nA placebo treated man aged 60 years who has prothrombin index 49 or below\n\n\nnewdata = data.frame(\n  sex = c(0,0,1,1), \n  treat = c(0,0,1,1), \n  age = rep(60, 4),\n  protgroup = c(3,1,3,1))\nnewdata\n\n  sex treat age protgroup\n1   0     0  60         3\n2   0     0  60         1\n3   1     1  60         3\n4   1     1  60         1\n\n\n\nsurv.res = survfit(cox.grfit, newdata = newdata)\nplot(surv.res, fun=\"cumhaz\", mark.time=FALSE, xlim=c(0,10),\nxlab=\"Years since operation\", ylab=\"Cumulative hazard\", lty=1:4, lwd=2, col = 1:4)\nlegend(\"topleft\",c(\"female, treated, 60y, progr = 3\",\"female, treated, 60y, protgr = 1\",\n\"male, placebo, 60y, progr = 3\",\"female, placebo, 60y, protgr = 1\"), lty=1:4,lwd=2, col = 1:4)\n\n\n\n\n\n\n\n\n\nplot(surv.res, fun=\"surv\", mark.time=FALSE, xlim=c(0,10),\nxlab=\"Years since operation\", ylab=\"Survival Probability\", lty=1:4, lwd=2, col = 1:4)\nlegend(\"topleft\",c(\"female, treated, 60y, progr = 3\",\"female, treated, 60y, protgr = 1\",\n\"male, placebo, 60y, progr = 3\",\"female, placebo, 60y, protgr = 1\"), lty=1:4,lwd=2, col = 1:4)",
    "crumbs": [
      "Cox Regression (low-dim)"
    ]
  },
  {
    "objectID": "cox_regr_high_dim.html",
    "href": "cox_regr_high_dim.html",
    "title": "Cox regression (high-dim)",
    "section": "",
    "text": "library(survival)\nlibrary(glmnet)\nlibrary(tibble)\nlibrary(dplyr)\n\nIn this exercise, we will use data for \\(115\\) Norwegian women with breast cancer. For each of the women, we have gene expression measurements for \\(549\\) «intrinsic genes» and information on the time to breast cancer death or censoring. (Source: Sørlie et al. Proc. Natl Acad. Sci. USA, 2003.)\nWe will use penalized Cox regression to investigate if the gene expressions have predictive power for survival and try to identify which genes are of importance.\nYou may read the data into R by the command:\n\nbrcancer=read.table(\"https://www.med.uio.no/imb/english/research/centres/ocbe/courses/imb9335/norw-breast-cancer.txt\")\n\nBefore we analyse the data, we will define 10 folds that may be used in the cross-validation (to ensure – for pedagogical reasons - that all students get the same results). We do this by the commands\n\nset.seed(33575)\nind=sample(1:115,115)\nbrcancer=brcancer[ind,]\nfold=rep(1:10,length.out=115)\n\nWe extract the (censored) survival times, the censoring/death statuses and the gene expressions by the commands\n\ntime=brcancer[,1]\nstatus=brcancer[,2]\ngeneexpr=as.matrix(brcancer[,-(1:2)])\n\nCensoring problems with the folds?\n\nsum(status == 0)/length(status)\n\n[1] 0.6695652\n\nstatus[fold == 5]\n\n [1] 0 0 0 0 0 1 0 0 0 0 0 0\n\n\nWe may then do a Cox-lasso regression with 10-fold cross-validation and plot the cross-validation curve by the commands (you need to load the survival and glmnet-packages)\n\ncox.lasso=cv.glmnet(geneexpr,Surv(time,status),family=\"cox\",foldid=fold, standardize=FALSE)\nplot(cox.lasso)\n\n\n\n\n\n\n\n\n\ncox.lasso$lambda.min\n\n[1] 0.2902299\n\nlog(cox.lasso$lambda.min)\n\n[1] -1.237082\n\nmin(cox.lasso$cvm)\n\n[1] 8.45301\n\n\n\nPerform the commands and interpret the cross-validation plot.\n\nWe may use the following commands to obtain a list of the genes that obtain an estimate that differ from zero (i.e. the selected genes):\n\ncoefficients=coef(cox.lasso, s=cox.lasso$lambda.min)\nactive.index=which(coefficients != 0)\nactive.coefficients=coefficients[active.index]\ncovarno=predict(cox.lasso, s=cox.lasso$lambda.min,type=\"nonzero\")\ncbind(covarno,active.coefficients)\n\n  which active.coefficients\n1   356          -0.1720938\n\n\nRepeat a number of times and see how the number of genes and the list of selected genes vary from splits to splits:\n\nn = 25\ndl = list()\nfor(i in 1:n) {\n  # message(i)\n  cox.lasso = cv.glmnet(geneexpr,Surv(time,status),family=\"cox\",nfolds=10, standardize=FALSE)\n  coef_tbl = coef(cox.lasso, s = 'lambda.min') %&gt;% # s = 'lambda.' different\n    as.matrix() %&gt;%\n    as.data.frame() %&gt;%\n    tibble::rownames_to_column(var = 'coef_name') %&gt;%\n    dplyr::rename(value = `1`) %&gt;%\n    filter(value != 0)\n  dl[[i]] = coef_tbl\n}\n\nres = dplyr::bind_rows(dl)\n\n\nres %&gt;% \n  group_by(coef_name) %&gt;% \n  summarise(mean_coef = mean(value), ntimes=n()) %&gt;%\n  arrange(desc(ntimes))\n\n# A tibble: 5 × 3\n  coef_name mean_coef ntimes\n  &lt;chr&gt;         &lt;dbl&gt;  &lt;int&gt;\n1 V358       -0.174       25\n2 V23        -0.0384      18\n3 V200       -0.0193       6\n4 V271       -0.00971      3\n5 V233        0.00290      1\n\n\nFinally, if time allows, you may try out the elastic net. You may fit a Cox elastic net (with alpha=0.5) for the original split into 10 folds by the command:\n\ncox.net=cv.glmnet(geneexpr,Surv(time,status),family=\"cox\",alpha=0.5,foldid=fold, standardize=FALSE)\nplot(cox.net)\n\n\n\n\n\n\n\n\n\nPerform the command and see how many and which genes are selected.",
    "crumbs": [
      "Cox regression (high-dim)"
    ]
  },
  {
    "objectID": "aalen.html",
    "href": "aalen.html",
    "title": "Additive regression",
    "section": "",
    "text": "library(survival)\nlibrary(tibble)\nlibrary(dplyr)\n\nAt the university hospital of the University of Massachusetts one has for a number of years studied the survival of patients admitted with an acute myocardial infarction (AMI- aka HEART ATTACK!). One aim of the study has been to investigate whether the survival of AMI patients has improved over time. A number of covariates were measured at hospitalization. In addition to information on time of hospitalization (here given in five years periods), we will in this problem restrict attention to the two demographic covariates age and sex, one covariate that is related to the seriousness of the AMI (amount of “heart enzyme”), and one covariate that gives information on whether the patient has had an AMI earlier or not. Further we will restrict attention to the first three years after AMI, so patients who live longer than \\(1095 = 3 \\times 365\\) days have been censored at that time.\nYou may read the AMI-data into R by the command:\n\nami=read.table(\"https://www.med.uio.no/imb/english/research/centres/ocbe/courses/imb9335/ami.txt\",header=T) %&gt;% as_tibble()\nami\n\n# A tibble: 481 × 8\n      id  days status   per   age   sex enzym  prev\n   &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     1     1      1     1    62     1   485     0\n 2     2     1      1     1    78     1   910     1\n 3     3     1      1     1    81     1   320     0\n 4     4     1      1     1    79     1  3290     1\n 5     5     2      1     1    60     1  2500     1\n 6     6     2      1     1    72     0    99     0\n 7     7     2      1     1    60     1  1200     0\n 8     8     3      1     1    83     1   160     0\n 9     9     3      1     1    78     0    66     1\n10    10  1095      0     1    72     1    99     0\n# ℹ 471 more rows\n\n\nThe data are organized with one line for each of the \\(481\\) patients, and with the following variables in the seven columns:\n\nid: Patient number\ndays: Number of days from hospitalization to death or censoring\nstatus: Indicator for death (1) or censoring (0)\nper: Five year period (1 = 1975-79, 2 = 1980-84, 3 = 1985-89)\nage: Age at hospitalization (in years)\nsex: Sex (0 = male, 1 = female)\nenzym: Amount of “heart enzyme” measured as ”international units” divided by 100\nprev: Information on earlier AMIs (0 = no earlier AMI, 1 = at least one earlier AMI).\n\nWe will study the effect of the five covariates (per, age, sex, enzym and prev) using the additive regression model. Our main interest lies in a simultaneous analysis of all five covariates. But nevertheless we will start out by fitting additive regression models with one covariate at a time.\n\nFirst we fit an additive model with sex as the only covariate and plot the estimated cumulative regression function. This is achieved by the commands:\n\n\nfit.sex = survival::aareg(Surv(days,status) ~ factor(sex), data=ami)\nfit.sex\n\nCall:\nsurvival::aareg(formula = Surv(days, status) ~ factor(sex), data = ami)\n\n  n= 481 \n    115 out of 115 unique event times used\n\n                slope    coef se(coef)    z        p\nIntercept    0.000734 0.00219 0.000222 9.85 6.93e-23\nfactor(sex)1 0.000405 0.00102 0.000407 2.51 1.20e-02\n\nChisq=6.31 on 1 df, p=0.012; test weights=aalen\n\nprint(fit.sex, maxtime = 100)\n\nCall:\nsurvival::aareg(formula = Surv(days, status) ~ factor(sex), data = ami)\n\n  n= 481 \n    44 out of 115 unique event times used\n\n               slope    coef se(coef)    z        p\nIntercept    0.00380 0.00186 0.000251 7.42 1.21e-13\nfactor(sex)1 0.00285 0.00124 0.000478 2.60 9.27e-03\n\nChisq=6.77 on 1 df, p=0.00927; test weights=aalen\n\n\nAlso test the null hypothesis that there is no effect of sex (marginally, i.e. when not corrected for the other covariates). You obtain the standardized test and its p-value from the last two columns of the output from the command print(fit.sex). (The maxtime option also works for the print command.)\n\nres = summary(fit.sex) # , maxtime = 100\nres$table[2,'p'] # borderline\n\n[1] 0.01200272\n\n\nIntercept figure always shows the baseline hazard (reference category when modeling with one covariate or all reference values when modeling with more covariates):\n\npar(mfrow=c(1,2))\nplot(fit.sex)\n\n\n\n\n\n\n\n\nYou may want to focus on the first 100 days after hospitalizations.\n\npar(mfrow=c(1,2))\nplot(fit.sex, maxtime = 100)\n\n\n\n\n\n\n\n\nEffect of sex variable is stronger the first 100 days (p-value):\n\nres = summary(fit.sex, maxtime = 100)\nres$table[2,'p']\n\n[1] 0.009270794\n\n\n\nFit an additive regression model for each of the other four covariates per, age, enzym and prev (one at a time). For each of the covariates you should interpret the estimates you obtain for the cumulative baseline and the cumulative regression function and decide whether the covariate has a significant effect (when not corrected for the effects of the other covariates). In order to ease interpretation of the estimated cumulative baseline function, you should center the numeric covariates age and enzym by subtracting their means.\n\nScale continuous variables:\n\nNote that the centering value (e.g. mean or other) is like the reference value:\n\n\nami2 = ami %&gt;% \n  mutate(age = (age - mean(age))/sd(age), enzym = (enzym - mean(enzym))/sd(enzym))\n\n\nper (hospitalization period) has no effect:\n\n\nfit.per = aareg(formula = Surv(days, status) ~ factor(per), data = ami)\nfit.per\n\nCall:\naareg(formula = Surv(days, status) ~ factor(per), data = ami)\n\n  n= 481 \n    115 out of 115 unique event times used\n\n                 slope      coef se(coef)      z        p\nIntercept     8.66e-04  0.002540 0.000328  7.750 9.49e-15\nfactor(per)2  1.52e-05  0.000196 0.000458  0.428 6.69e-01\nfactor(per)3 -6.25e-05 -0.000104 0.000471 -0.222 8.25e-01\n\nChisq=0.43 on 2 df, p=0.805; test weights=aalen\n\npar(mfrow = c(1,3))\nplot(fit.per)\n\n\n\n\n\n\n\n\n\nfit.age = aareg(formula = Surv(days, status) ~ age, data = ami)\nfit.age\n\nCall:\naareg(formula = Surv(days, status) ~ age, data = ami)\n\n  n= 481 \n    115 out of 115 unique event times used\n\n              slope      coef se(coef)     z        p\nIntercept -1.74e-03 -0.005130 1.00e-03 -5.13 2.90e-07\nage        4.22e-05  0.000116 1.64e-05  7.10 1.27e-12\n\nChisq=50.38 on 1 df, p=1.27e-12; test weights=aalen\n\npar(mfrow = c(1,2))\nplot(fit.age)\n\n\n\n\n\n\n\n\n\nScaling/Centering continuous variable age is important for the interpretation of the baseline hazard!!!\nHazard increases with larger age! (there is an effect)\n\n\nfit.age2 = aareg(formula = Surv(days, status) ~ age, data = ami2)\nfit.age2\n\nCall:\naareg(formula = Surv(days, status) ~ age, data = ami2)\n\n  n= 481 \n    115 out of 115 unique event times used\n\n             slope    coef se(coef)    z        p\nIntercept 0.001010 0.00275 0.000204 13.4 3.46e-41\nage       0.000535 0.00147 0.000208  7.1 1.27e-12\n\nChisq=50.38 on 1 df, p=1.27e-12; test weights=aalen\n\npar(mfrow = c(1,2))\nplot(fit.age2)\n\n\n\n\n\n\n\n\n\nenzym has no significant effect:\n\n\nfit.enzym = aareg(formula = Surv(days, status) ~ enzym, data = ami2)\nfit.enzym\n\nCall:\naareg(formula = Surv(days, status) ~ enzym, data = ami2)\n\n  n= 481 \n    115 out of 115 unique event times used\n\n             slope     coef se(coef)     z        p\nIntercept 0.000909 0.002580 0.000190 13.50 1.10e-41\nenzym     0.000223 0.000444 0.000269  1.65 9.92e-02\n\nChisq=2.72 on 1 df, p=0.0992; test weights=aalen\n\npar(mfrow = c(1,2))\nplot(fit.enzym)\n\n\n\n\n\n\n\n\n\nprev =&gt; previous hospitalization has significant effect:\n\n\nfit.prev = aareg(formula = Surv(days, status) ~ factor(prev), data = ami)\nfit.prev\n\nCall:\naareg(formula = Surv(days, status) ~ factor(prev), data = ami)\n\n  n= 481 \n    115 out of 115 unique event times used\n\n                 slope    coef se(coef)    z        p\nIntercept     0.000654 0.00203 0.000207 9.85 6.93e-23\nfactor(prev)1 0.000674 0.00166 0.000444 3.75 1.77e-04\n\nChisq=14.06 on 1 df, p=0.000177; test weights=aalen\n\npar(mfrow = c(1,2))\nplot(fit.prev)\n\n\n\n\n\n\n\n\n\nFit an additive regression model with all the five covariates. Determine which of covariates have a significant effect on the mortality (using “backwards elimination”), and thereby obtain a “final model” where all covariates have a significant effect. Interpret the estimates of the cumulative baseline and the cumulative regression functions for your “final model”.\n\n\nfit.all = aareg(formula = Surv(days, status) ~ factor(per) + age + factor(sex) + enzym + factor(prev), data = ami2)\nfit.all\n\nCall:\naareg(formula = Surv(days, status) ~ factor(per) + age + factor(sex) + \n    enzym + factor(prev), data = ami2)\n\n  n= 481 \n    115 out of 115 unique event times used\n\n                  slope      coef se(coef)      z        p\nIntercept      0.000842  0.002390 0.000425  5.630 1.83e-08\nfactor(per)2  -0.000140 -0.000176 0.000477 -0.368 7.13e-01\nfactor(per)3  -0.000297 -0.000681 0.000474 -1.440 1.50e-01\nage            0.000552  0.001480 0.000222  6.680 2.44e-11\nfactor(sex)1   0.000208  0.000389 0.000426  0.914 3.61e-01\nenzym          0.000387  0.000873 0.000283  3.090 2.01e-03\nfactor(prev)1  0.000647  0.001560 0.000454  3.430 5.98e-04\n\nChisq=67.78 on 6 df, p=1.16e-12; test weights=aalen\n\npar(mfrow = c(2,4))\nplot(fit.all)\n\n\n\n\n\n\n\n\n\nprint(fit.all, maxtime = 100)\n\nCall:\naareg(formula = Surv(days, status) ~ factor(per) + age + factor(sex) + \n    enzym + factor(prev), data = ami2)\n\n  n= 481 \n    44 out of 115 unique event times used\n\n                 slope      coef se(coef)     z        p\nIntercept      0.00519  0.002180 0.000481  4.53 5.91e-06\nfactor(per)2  -0.00273 -0.000834 0.000552 -1.51 1.31e-01\nfactor(per)3  -0.00206 -0.000728 0.000568 -1.28 1.99e-01\nage            0.00293  0.001260 0.000254  4.98 6.50e-07\nfactor(sex)1   0.00217  0.000874 0.000497  1.76 7.87e-02\nenzym          0.00388  0.001350 0.000395  3.42 6.27e-04\nfactor(prev)1  0.00277  0.001390 0.000514  2.71 6.77e-03\n\nChisq=46.16 on 6 df, p=2.75e-08; test weights=aalen\n\npar(mfrow = c(2,4))\nplot(fit.all, maxtime = 100)\n\n\n\n\n\n\n\n\nLet’s keep only the variables in the model that had a significant effect:\n\nfit.final = aareg(formula = Surv(days, status) ~ age + enzym + factor(prev), data = ami2)\nfit.final\n\nCall:\naareg(formula = Surv(days, status) ~ age + enzym + factor(prev), \n    data = ami2)\n\n  n= 481 \n    115 out of 115 unique event times used\n\n                 slope     coef se(coef)     z        p\nIntercept     0.000752 0.002270 0.000224 10.10 3.81e-24\nage           0.000554 0.001510 0.000212  7.10 1.23e-12\nenzym         0.000358 0.000812 0.000279  2.90 3.67e-03\nfactor(prev)1 0.000653 0.001570 0.000450  3.49 4.85e-04\n\nChisq=62.99 on 3 df, p=1.35e-13; test weights=aalen\n\npar(mfrow = c(2,2))\nplot(fit.final)\n\n\n\n\n\n\n\n\n\nWhen analyzing censored survival data, Cox regression is commonly used. Discuss benefits and drawbacks of using the additive model (rather than Cox’s model) for analyzing the AMI data.\n\n\nWith additive regression you can check what is the treatment effect across time, which you lose when doing CoxPH!",
    "crumbs": [
      "Additive regression"
    ]
  },
  {
    "objectID": "mstate.html",
    "href": "mstate.html",
    "title": "Multi-state models",
    "section": "",
    "text": "library(survival)\nlibrary(mstate)\nlibrary(tibble)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nIn this exercise, we will see how we may find the Nelson-Aalen estimates for the cumulative transition intensities and the Aalen-Johansen estimates for the transition probabilities in a Markov illness-death model. To this end, we will use the bone marrow transplantation data described in example 1.13 in the ABG-book and used for illustration in example 3.16. To read the data into R you may give the command:\n\nbonemarrow = read.table(\"https://www.med.uio.no/imb/english/research/centres/ocbe/courses/imb9335/bone-marrow.txt\",header=T) %&gt;% as_tibble()\nbonemarrow %&gt;% select(T2, DF, TP, P)\n\n# A tibble: 137 × 4\n      T2    DF    TP     P\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n 1  2081     0    13     1\n 2  1602     0    18     1\n 3  1496     0    12     1\n 4  1462     0    13     1\n 5  1433     0    12     1\n 6  1377     0    12     1\n 7  1330     0    17     1\n 8   996     0    12     1\n 9   226     0    10     1\n10  1199     0    29     1\n# ℹ 127 more rows\n\n\nBone marrow transplantation data:\n\ng =&gt; Disease Group (1: ALL, 2: AML Low Risk, 3: AML High Risk)\nT1 =&gt; Time To Death Or On Study Time\nT2 =&gt; Disease Free Survival Time (Time To Relapse, Death Or End Of Study)\nD =&gt; Death Indicator (1: Dead, 0: Alive)\nR =&gt; Relapse Indicator (1: Relapsed, 0: Disease Free)\nDF =&gt; Disease Free Survival Indicator (1: Dead Or Relapsed, 0: Alive Disease Free)\nTA =&gt; Time To Acute Graft-Versus-Host Disease\nA =&gt; Acute GVHD Indicator (1: Developed Acute GVHD, 0: Never Developed Acute GVHD)\nTC =&gt; Time To Chronic Graft-Versus-Host Disease\nC =&gt; Chronic GVHD Indicator (1-Developed Chronic GVHD, 0:Never Developed Chronic GVHD)\nTP =&gt; Time To Return of Platelets to Normal Levels\nP =&gt; Platelet Recovery Indicator (1: Platelets Returned To Normal, 0-Platelets Never Returned to Normal)\nZ1 =&gt; Patient Age In Years\nZ2 =&gt; Donor Age In Years\nZ3 =&gt; Patient Sex (1-Male, 0-Female)\nZ4 =&gt; Donor Sex (1: Male, 0: Female)\nZ5 =&gt; Patient CMV Status (1: CMV Positive, 0: CMV Negative)\nZ6 =&gt; Donor CMV Status (1: CMV Positive, 0: CMV Negative)\nZ7 =&gt; Waiting Time to Transplant In Days\nZ8 =&gt; FAB (1: FAB Grade 4 Or 5 and AML, 0: Otherwise)\nZ9 =&gt; Hospital (1:The Ohio State University, 2: Alferd , 3: St. Vincent, 4: Hahnemann)\nZ10 =&gt; MTX Used as a Graft-Versus-Host- Prophylactic (1:Yes, 0:No)\n\nThe data contain information for \\(137\\) patients with acute leukemia who had a bone marrow transplantation. The patients were followed for a maximum of seven years, and times to relapse and death were recorded. It was also recorded if and when the platelet count of a patient returned to a self-sustaining level. The possible events for a patient may be described by an illness-death model without recovery with the three states “transplanted”, “platelet recovered”, and “relapsed or dead”. A patient starts out in state “transplanted” at time zero when he/she gets the bone marrow transplant. If the platelets recover, the patient moves to state “platelet recovered”, and if he/she then relapses or dies, the patient moves on to state “relapsed or dead”. If the patient relapses or dies without the platelets returning to a normal level, he moves directly from state “transplanted” to state “relapsed or dead”.\nTo find the Nelson-Aalen estimates and the empirical transition probabilities (i.e. Aalen-Johansen estimates) we will use the mstate package so this has to be installed and loaded.\nWe start out by defining the states and the possible transitions between them. This is achieved by the command:\n\ntmat = transMat(x = list(c(2,3),c(3),c()), names = c(\"transplanted\",\"recovered\",\"relapsed.dead\"))\ntmat\n\n               to\nfrom            transplanted recovered relapsed.dead\n  transplanted            NA         1             2\n  recovered               NA        NA             3\n  relapsed.dead           NA        NA            NA\n\n\nTo perform the estimation, we need to convert the data-frame to a long format, where there are 2-3 lines for each patient:\n\nbonemarrow.long = msprep(time=c(NA,\"TP\",\"T2\"), status=c(NA,\"P\",\"DF\"),data=bonemarrow,trans=tmat)\nbonemarrow.long %&gt;% as_tibble()\n\n# A tibble: 394 × 8\n      id  from    to trans Tstart Tstop  time status\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     1     1     2     1      0    13    13      1\n 2     1     1     3     2      0    13    13      0\n 3     1     2     3     3     13  2081  2068      0\n 4     2     1     2     1      0    18    18      1\n 5     2     1     3     2      0    18    18      0\n 6     2     2     3     3     18  1602  1584      0\n 7     3     1     2     1      0    12    12      1\n 8     3     1     3     2      0    12    12      0\n 9     3     2     3     3     12  1496  1484      0\n10     4     1     2     1      0    13    13      1\n# ℹ 384 more rows\n\n\n\n# example\nbonemarrow %&gt;% as_tibble() %&gt;% filter(T2 == 383)\n\n# A tibble: 1 × 22\n      g    T1    T2     D     R    DF    TA     A    TC     C    TP     P    Z1\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n1     1   417   383     1     1     1   417     0   417     0    16     1    15\n# ℹ 9 more variables: Z2 &lt;int&gt;, Z3 &lt;int&gt;, Z4 &lt;int&gt;, Z5 &lt;int&gt;, Z6 &lt;int&gt;,\n#   Z7 &lt;int&gt;, Z8 &lt;int&gt;, Z9 &lt;int&gt;, Z10 &lt;int&gt;\n\n\nThen we fit a stratified Cox-model with no covariates (stratifying on the type of transition), extract the Nelson-Aalen estimates for the cumulative transition intensities and make a plot of these:\n\ncox.bm=coxph(Surv(Tstart,Tstop,status)~strata(trans),data=bonemarrow.long, method=\"breslow\")\nhaz.bm=msfit(cox.bm,trans=tmat)\n\nplot(haz.bm,xlab=\"Days post-transplant\",xlim=c(0,1000),ylim=c(0,3.5))\n\n\n\n\n\n\n\n\nTo find the Aalen-Johansen estimates of the transition probabilities, we give the command:\n\nprob.bm = probtrans(haz.bm,predt=0)\n\nWe may extract the empirical transition probabilities from state “transplanted” (state 1) by the command prob.bm[[1]], and similarly we obtain the empirical transition probabilities from state “recovered” (state 2) by prob.bm[[2]].\nThe transition probabilities may be plotted in a stacked manner or as separate estimates by the commands:\n\nplot(prob.bm,type=\"filled\",ord=c(2,1,3))\n\n\n\n\n\n\n\nplot(prob.bm,type=\"filled\")\n\n\n\n\n\n\n\n\nProbability of being in each state:\n\nplot(prob.bm,type=\"single\",xlim=c(0,1000),col=1:3)\n\n\n\n\n\n\n\n\n\nPerform the commands given above. Make sure that you understand what each of the commands and plots give you, and compare the results with those of example 3.16 in the ABG-book.\nAbove we used the probtrans command with the option predt=0. This gives transition probabilities from time s = 0 to time t. Also compute and plot the transition probabilities for \\(s = 14, 28\\) and \\(42\\). What can you learn from these plots?\n\n\nprob.bm.14 = probtrans(haz.bm,predt=14)\nplot(prob.bm.14, type = \"single\", xlim = c(0,1000), col = 1:3, main = 's = 14')\n\n\n\n\n\n\n\n\n\nprob.bm.28 = probtrans(haz.bm, predt = 28)\nplot(prob.bm.28, type = \"single\", xlim = c(0,1000), col = 1:3, main = 's = 28')\n\n\n\n\n\n\n\n\n\nprob.bm.42 = probtrans(haz.bm,predt=42)\nplot(prob.bm.42, type = \"single\", xlim = c(0,1000), col = 1:3, main = 's = 42')\n\n\n\n\n\n\n\n\n\nWhat if I start from state 2?\n\n\nplot(prob.bm.14, from = 2, type = \"single\", xlim = c(0,1000), col = 1:3, main = 's = 14 from 2nd Recovery state')\n\n\n\n\n\n\n\n\nReference:\n\nKlein, JP and Moeschberger, ML (2003) Survival analysis. Techniques for Censored and Truncated Data (2nd edition). Springer-Verlag, New York.",
    "crumbs": [
      "Multi-state models"
    ]
  },
  {
    "objectID": "frailty.html",
    "href": "frailty.html",
    "title": "Frailty models for clustered data",
    "section": "",
    "text": "library(survival)\nlibrary(parfm)\nlibrary(tibble)\nlibrary(dplyr)\n\nThe Diabetic Retinopathy Study was conducted in the US by the National Eye Institute to assess the effect of laser photocoagulation in delaying onset of severe visual loss (“blindness”) in patients with diabetic retinopathy. One eye of each patient was randomly selected for photocoagulation and the other was observed without treatment. The patients were followed over several years for the occurrence of blindness (event = blindness) in their left and right eyes. Censoring is caused by death, dropout, or end of the study. We consider only a subset of the original data set containing \\(197\\) high risk patients. See Huster et al, 1989, Biometrics, for a discussion and further references.\nThere are two lines for each patient in the data set: one line per eye. The variables are:\n\nid: patient number\ntrteye: treated eye (1=right, 2=left)\nageonset: age at diagnosis of diabetes\ntypediab: type of diabetes (1= juvenile age at onset &lt; 20 years , 2=adult)\ntime: follow-up time in months\nstatus: status for eye (0=censored, 1=blindness)\ntrt: treatment of eye (1=control eye, 2=treated eye)\n\nBefore you start, you will have to read the data. Use the R-command:\n\neye=read.table(\"https://www.med.uio.no/imb/english/research/centres/ocbe/courses/imb9335/retinopathy.txt\", header=T) %&gt;% as_tibble()\neye\n\n# A tibble: 394 × 7\n      id trteye ageonset typediab status  time   trt\n   &lt;int&gt;  &lt;int&gt;    &lt;int&gt;    &lt;int&gt;  &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n 1     5      2       28        2      0 46.2      1\n 2    14      1       12        1      1 31.3      1\n 3    16      1        9        1      0 42.3      1\n 4    25      2        9        1      0 20.6      1\n 5    29      2       13        1      1  0.3      1\n 6    46      1       12        1      1 54.3      1\n 7    49      1        8        1      1 10.8      1\n 8    56      1       12        1      0 23.2      1\n 9    61      1       16        1      0  1.47     1\n10    71      1       21        2      1 13.8      1\n# ℹ 384 more rows\n\n\nWe start out by only using treatment as a covariate.\n\nWe first fit a model with Weibull baseline and no frailty by the command:\n\n\nnofrail.model = parfm(Surv(time,status)~factor(trt),data=eye)\nnofrail.model\n\n\nFrailty distribution: none \nBaseline hazard distribution: Weibull \nLoglikelihood: -836.379 \n\n             ESTIMATE SE    p-val    \nrho           0.810   0.058          \nlambda        0.032   0.008          \nfactor(trt)2 -0.790   0.169 &lt;.001 ***\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nci.parfm(nofrail.model)\n\n               low    up\nfactor(trt)2 0.326 0.632\n\n\nPerform the command, and make sure that you understand what the output tells you.\n\nWe then fit a model with Weibull baseline and gamma frailty:\n\n\nfrail.model = parfm(Surv(time,status)~factor(trt),cluster=\"id\", frailty=\"gamma\", data=eye)\nfrail.model\n\n\nFrailty distribution: gamma \nBaseline hazard distribution: Weibull \nLoglikelihood: -827.744 \n\n             ESTIMATE SE    p-val    \ntheta         1.040   0.329          \nrho           0.948   0.075          \nlambda        0.028   0.007          \nfactor(trt)2 -0.960   0.182 &lt;.001 ***\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nKendall's Tau: 0.342 \n\n\nPerform the command, and compare with the result for the model without frailty. Is there a significant frailty effect?\n\nLR=2*(attributes(frail.model)$loglik - attributes(nofrail.model)$loglik)\nLR\n\n[1] 17.26943\n\n(1-pchisq(LR, 1))/2 # YES!\n\n[1] 1.621814e-05\n\n\n\nSignificant risk variation between the clusters (individuals here)!\n\n\nFit a model with gamma frailty and treatment and age at onset as covariates. Is there a significant effect of age at onset? What about type of diabetes?\n\n\nfrail.model.ageonset = parfm(Surv(time,status)~factor(trt) + ageonset, cluster=\"id\", frailty=\"gamma\", data=eye)\nfrail.model.ageonset # NO! (coef close to 0, p-value large)\n\n\nFrailty distribution: gamma \nBaseline hazard distribution: Weibull \nLoglikelihood: -827.469 \n\n             ESTIMATE SE    p-val    \ntheta         1.034   0.328          \nrho           0.948   0.075          \nlambda        0.025   0.008          \nfactor(trt)2 -0.966   0.182 &lt;.001 ***\nageonset      0.006   0.008 0.456    \n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nKendall's Tau: 0.341 \n\nfrail.model.typediab = parfm(Surv(time,status)~factor(trt) + factor(typediab), cluster=\"id\", frailty=\"gamma\", data=eye)\nfrail.model.typediab # NO! (coef close to 0, p-value large)\n\n\nFrailty distribution: gamma \nBaseline hazard distribution: Weibull \nLoglikelihood: -827.73 \n\n                  ESTIMATE SE    p-val    \ntheta              1.037   0.329          \nrho                0.947   0.075          \nlambda             0.028   0.008          \nfactor(trt)2      -0.961   0.182 &lt;.001 ***\nfactor(typediab)2  0.040   0.232 0.864    \n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nKendall's Tau: 0.341 \n\n\n\nFit a Cox frailty model (with gamma frailty) using treatment as the only covariate (see lectures for R commands). Compare the results with those you obtained in question b.\n\n\nfrail.cox = coxph(Surv(time,status) ~ factor(trt) + frailty(id), data=eye)\nfrail.cox\n\nCall:\ncoxph(formula = Surv(time, status) ~ factor(trt) + frailty(id), \n    data = eye)\n\n                coef se(coef)     se2   Chisq   DF       p\nfactor(trt)2  -0.910    0.174   0.171  27.295  1.0 1.7e-07\nfrailty(id)                           114.448 84.6   0.017\n\nIterations: 6 outer, 30 Newton-Raphson\n     Variance of random effect= 0.854   I-likelihood = -850.9 \nDegrees of freedom for terms=  1.0 84.6 \nLikelihood ratio test=201  on 85.6 df, p=3e-11\nn= 394, number of events= 155 \n\n\n\n# cox with no frailty\ncox = coxph(Surv(time, status) ~ factor(trt), data = eye)\n\nLRcox = 2*(frail.cox$history$frailty$c.loglik - cox$loglik[2])\nLRcox # log-likelihood ratio\n\n[1] 11.88032\n\n1 - pchisq(LRcox,df=1)\n\n[1] 0.0005673029\n\n\n\nFinally we want to illustrate how stratified Cox regression can be used to analyse paired survival data like the ones we have in the present situation. We may fit a stratified Cox model with a separate baseline for each patient (per eye pair pretty much) by the commands:\n\n\neyefit.strat=coxph(Surv(time,status)~trt+strata(id),data=eye)\nsummary(eyefit.strat)\n\nCall:\ncoxph(formula = Surv(time, status) ~ trt + strata(id), data = eye)\n\n  n= 394, number of events= 155 \n\n       coef exp(coef) se(coef)      z Pr(&gt;|z|)    \ntrt -0.9623    0.3820   0.2016 -4.773 1.82e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n    exp(coef) exp(-coef) lower .95 upper .95\ntrt     0.382      2.618    0.2573    0.5672\n\nConcordance= 0.748  (se = 0.058 )\nLikelihood ratio test= 25.49  on 1 df,   p=4e-07\nWald test            = 22.78  on 1 df,   p=2e-06\nScore (logrank) test = 24.59  on 1 df,   p=7e-07\n\n\nExplain the idea behind such an approach, and discuss how it compares to the approach in question d).\nWe got same approximately results! Why? See equation (slide no. 24): \\(a_{ij}(t|Z_i) = Z_i a_0(t) e^{\\beta x_{ij}}\\)\n\nCox model uses different baseline hazards per stratum (\\(Z_i=1\\))\nParametric Frailty models are better in most practical applications (\\(a_0(t)\\) same for all individuals, \\(Z_i\\) varies and is estimated)",
    "crumbs": [
      "Frailty models for clustered data"
    ]
  },
  {
    "objectID": "ncc.html",
    "href": "ncc.html",
    "title": "Case-control studies",
    "section": "",
    "text": "library(survival)\nlibrary(glmnet)\nlibrary(tibble)\nlibrary(dplyr)\n\nIn this exercise we will use data from a cohort of \\(1720\\) female patients who were discharged from two tuberculosis sanatoria in Massachusetts between 1930 and 1956 to investigate breast cancer risk of radiation exposure due to fluoroscopy. Radiation doses have been estimated for \\(1022\\) women who received radiation exposure to the chest from X-ray fluoroscopy lung examinations. Some women had multiple exposures to radiation. The remaining \\(698\\) women in the cohort received treatments that did not require fluoroscopic monitoring and were radiation unexposed. The patients had been followed up until the end of 1980, by which time \\(75\\) breast cancer cases were observed. (Source: Hrubec et al., Cancer Research, 1989)\nFor this cohort radiation data have been collected for all \\(1720\\) women. But the workload of exposure data collection would have been reduced if the investigators had used a cohort sampling design. In this exercise we will look at estimation based on the full cohort and for nested case-control and case-cohort samples selected from the full cohort.\nWe first look at the cohort data. You may read these data into R by the command\n\ncohort=read.table(\"https://www.med.uio.no/imb/english/research/centres/ocbe/courses/imb9335/radiationbreast.cohort.txt\", header=T) %&gt;% as_tibble()\ncohort\n\n# A tibble: 1,720 × 6\n      id   dose number ageentry ageexit status\n   &lt;int&gt;  &lt;dbl&gt;  &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;int&gt;\n 1     1 0           0     14.6    47.4      0\n 2     2 0           0     14.2    52.6      0\n 3     3 0           0     17.5    69.8      0\n 4     4 0           0     14.0    46.3      0\n 5     5 0           0     16.5    61.7      0\n 6     6 0.0660      4     16.0    58.6      0\n 7     7 1.25      108     13.0    62.3      0\n 8     8 0           0     13.2    53.5      0\n 9     9 0           0     17.3    42.5      1\n10    10 0.141      32     19.0    22.6      0\n# ℹ 1,710 more rows\n\n\nThe data are organized with one line for each of the 1720 women, and with the following variables in the six columns:\n\nid: subject id\ndose: radiation dose (Gy)\nnumber: number of fluoroscopic examinations\nageentry: age at entry into the study (in years)\nageexit: age at breast cancer or censoring (in years)\nstatus: breast cancer status (0: censored; 1: breast cancer)\n\nTo model the effect of radiation dose on breast cancer risk, we will consider a Cox regression model with \\(log2(dose + 1)\\), with dose measured in grays (Gy), as covariate. You may fit this model for cohort data by the commands:\n\nfit.cohort=coxph(Surv(ageentry,ageexit,status)~log2(dose+1), data=cohort)\nsummary(fit.cohort)\n\nCall:\ncoxph(formula = Surv(ageentry, ageexit, status) ~ log2(dose + \n    1), data = cohort)\n\n  n= 1720, number of events= 75 \n\n                 coef exp(coef) se(coef)     z Pr(&gt;|z|)   \nlog2(dose + 1) 0.4907    1.6335   0.1617 3.035  0.00241 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n               exp(coef) exp(-coef) lower .95 upper .95\nlog2(dose + 1)     1.634     0.6122      1.19     2.243\n\nConcordance= 0.586  (se = 0.034 )\nLikelihood ratio test= 8.64  on 1 df,   p=0.003\nWald test            = 9.21  on 1 df,   p=0.002\nScore (logrank) test = 9.44  on 1 df,   p=0.002\n\n\nWe then consider nested case-control data with two controls per case \\((m=2)\\). You may read these data into R by the command\n\nncc=read.table(\"https://www.med.uio.no/imb/english/research/centres/ocbe/courses/imb9335/radiationbreast.ncc.txt\", header=T) %&gt;% as_tibble()\nncc\n\n# A tibble: 225 × 8\n      id  dose number ageentry ageexit status setno  case\n   &lt;int&gt; &lt;dbl&gt;  &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     9 0          0    17.3     42.5      1     1     1\n 2  2717 0          0    31.2     83.2      0     1     0\n 3   311 0          0    14.4     61.7      0     1     0\n 4    22 0.617     39    17.2     25.4      1     2     1\n 5    66 0.259     27    11.5     44.8      0     2     0\n 6   401 1.83     111     5.25    51.4      0     2     0\n 7    29 0.594     36    17.7     48.5      1     3     1\n 8  3027 2.63     267    21.5     73.8      0     3     0\n 9  2349 0.807     82    23.4     69.7      0     3     0\n10    31 0          0    12.5     36.6      1     4     1\n# ℹ 215 more rows\n\n\nThe data are organized with three lines for each of the 75 sampled risk sets (one line for the case, and one line for each of the two controls, randomly selected, but falling within the risk set, e.g. the is an overlap between the age interval - think!), and with eight columns. The first six columns are as for the cohort data, while the variables in the two last columns are:\n\ncase: case-control status within the sampled risk set (0=control, 1=case)\nsetno: label of sampled risk set\n\nYou may fit the Cox regression model for the nested case-control data by the commands: Note we use case instead of status and strata(setno) so that only the 3 patients per setno are used for the calculation of the risk set:\n\nfit.ncc = coxph(Surv(ageentry,ageexit,case) ~ log2(dose+1)+strata(setno),data=ncc)\nsummary(fit.ncc)\n\nCall:\ncoxph(formula = Surv(ageentry, ageexit, case) ~ log2(dose + 1) + \n    strata(setno), data = ncc)\n\n  n= 225, number of events= 75 \n\n                 coef exp(coef) se(coef)     z Pr(&gt;|z|)  \nlog2(dose + 1) 0.5404    1.7167   0.2366 2.284   0.0224 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n               exp(coef) exp(-coef) lower .95 upper .95\nlog2(dose + 1)     1.717     0.5825      1.08      2.73\n\nConcordance= 0.563  (se = 0.058 )\nLikelihood ratio test= 5.39  on 1 df,   p=0.02\nWald test            = 5.22  on 1 df,   p=0.02\nScore (logrank) test = 5.39  on 1 df,   p=0.02\n\n\n\nExplain why these commands maximize the partial likelihood for nested case-control data. Perform the commands and compare the result with those you obtained for the cohort data.\n\nWe then consider case-cohort data with a subcohort of \\(150\\) women. You may read these data into R by the command\n\ncaseco=read.table(\"https://www.med.uio.no/imb/english/research/centres/ocbe/courses/imb9335/radiationbreast.caseco.txt\", header=T) %&gt;% as_tibble()\ncaseco\n\n# A tibble: 218 × 7\n      id  dose number ageentry ageexit status  subc\n   &lt;int&gt; &lt;dbl&gt;  &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;int&gt; &lt;int&gt;\n 1  2133 0          0     33.3    68.6      0     1\n 2  2608 0.826     84     25.8    67.6      0     1\n 3   497 0          0     16.2    60.6      0     1\n 4  2658 1.62     193     24.4    63.7      0     1\n 5  2611 0          0     24.2    68.5      0     1\n 6  3152 0          0     36.5    75.7      0     1\n 7   118 0.974     59     16.3    34.2      0     1\n 8  3352 0          0     27.9    39.3      1     1\n 9  2210 1.51     181     20.8    54.0      0     1\n10   165 0.248     15     10.2    19.1      0     1\n# ℹ 208 more rows\n\n\nThe data are organized with one line for each woman who is in the subcohort or is a cancer case outside the subcohort (no censored patients outside the subcohort!). The \\(150\\) women were randomly selected to be in the subcohort. The rest to reach \\(218\\) were the cases added (\\(status=1\\)) that are not in the subcohort. All the cases (\\(75\\)) are included in the data (as was the case with ncc)! The data file has seven columns. The first six columns are as for the cohort data, while the last column is:\n\nsubc: subcohort status (0: not in subcohort; 1: member of subcohort)\n\nFor case cohort-data there are two possible methods of estimation. You may fit the Cox model for the case-cohort data using Prentice’s method by the commands:\n\nfit.prentice = cch(Surv(ageentry,ageexit,status)~ log2(dose+1), data=caseco, subcoh=~subc, id=~id, method=\"Prentice\", cohort.size=1720)\n\nprint(fit.prentice)\n\nCase-cohort analysis,x$method, Prentice \n with subcohort of 150 from cohort of 1720 \n\nCall: cch(formula = Surv(ageentry, ageexit, status) ~ log2(dose + 1), \n    data = caseco, subcoh = ~subc, id = ~id, cohort.size = 1720, \n    method = \"Prentice\")\n\nCoefficients:\n         Value        SE        Z          p\n[1,] 0.5194255 0.2152382 2.413259 0.01581057\n\nsummary(fit.prentice)\n\nCase-cohort analysis,x$method, Prentice \n with subcohort of 150 from cohort of 1720 \n\nCall: cch(formula = Surv(ageentry, ageexit, status) ~ log2(dose + 1), \n    data = caseco, subcoh = ~subc, id = ~id, cohort.size = 1720, \n    method = \"Prentice\")\n\nCoefficients:\n       Coef    HR  (95%   CI)     p\nValue 0.519 1.681 1.102 2.563 0.016\n\n\n\nPerform the commands and compare the result with those you obtained for cohort data and nested case-control data. Also fit the model using inverse probability weighting (which is achieved by using method=“LinYing”), and compare with the results for Prentice’s method.\n\n\nfit.ipw = survival::cch(Surv(ageentry,ageexit,status)~ log2(dose+1), data=caseco, subcoh=~subc, id=~id, method=\"LinYing\", cohort.size=1720)\n\nsummary(fit.ipw)\n\nCase-cohort analysis,x$method, LinYing \n with subcohort of 150 from cohort of 1720 \n\nCall: survival::cch(formula = Surv(ageentry, ageexit, status) ~ log2(dose + \n    1), data = caseco, subcoh = ~subc, id = ~id, cohort.size = 1720, \n    method = \"LinYing\")\n\nCoefficients:\n       Coef    HR  (95%   CI)     p\nValue 0.524 1.688 1.116 2.555 0.013\n\n\nFinally we consider stratified case-cohort data where the sub-cohort consists of 50 women selected from each of the three strata:\n\nwomen with no fluoroscopic examinations (698 women)\nwomen with 1-149 fluoroscopic examinations (765 women)\nwomen with 150 or more fluoroscopic examinations (257 women)\n\nYou may read the stratified case-cohort data into R by the command:\n\nstratcaseco=read.table(\"https://www.med.uio.no/imb/english/research/centres/ocbe/courses/imb9335/radiationbreast.stratcaseco.txt\", header=T) %&gt;% as_tibble()\nstratcaseco\n\n# A tibble: 221 × 8\n      id  dose number ageentry ageexit status  subc stratum\n   &lt;int&gt; &lt;dbl&gt;  &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;int&gt; &lt;int&gt;   &lt;int&gt;\n 1  2894     0      0     29.3    39.8      0     1       1\n 2  2885     0      0     40.6    76.4      0     1       1\n 3  2490     0      0     18.3    40.9      0     1       1\n 4  2717     0      0     31.2    83.2      0     1       1\n 5  3015     0      0     29.7    60.4      0     1       1\n 6    73     0      0     16.3    64.6      0     1       1\n 7    44     0      0     17.9    19.5      0     1       1\n 8  3277     0      0     26.1    70.4      0     1       1\n 9  2873     0      0     25.6    62.9      0     1       1\n10  2423     0      0     24.8    62.1      0     1       1\n# ℹ 211 more rows\n\n\nThe data are organized with one line for each woman who is in the subcohort or is a cancer case outside the subcohort. The data file has eight columns. The first six columns are as for the cohort data, while the two last column are\n\nsubc: subcohort status (0: not in subcohrt; 1: member of subcohort)\nstratum: sampling stratum (1: no fluoroscopic examinations; 2: 1-149 examinations; 3: 150 examinations or more)\n\nYou may fit the Cox model for the stratified case-cohort data using inverse probability weighting by the commands:\n\nstratsize=c(698,765,257)\nnames(stratsize)=c(1,2,3)\nfit.ipw2=cch(Surv(ageentry,ageexit,status)~log2(dose+1), data=stratcaseco, subcoh=~subc,id=~id, stratum=~stratum, method=\"II.Borgan\", cohort.size=stratsize)\n# Borgan II method is a generalization of the Lin-Ying estimator for stratified case-control data\nsummary(fit.ipw2)\n\nExposure-stratified case-cohort analysis, II.Borgan method.\n            1   2   3\nsubcohort  71  87  63\ncohort    698 765 257\nCall: cch(formula = Surv(ageentry, ageexit, status) ~ log2(dose + 1), \n    data = stratcaseco, subcoh = ~subc, id = ~id, stratum = ~stratum, \n    cohort.size = stratsize, method = \"II.Borgan\")\n\nCoefficients:\n       Coef    HR  (95%   CI)     p\nValue 0.506 1.658 1.157 2.377 0.006\n\n\n\nPerform the commands and compare the result with those you obtained for case-cohort data with no stratification.",
    "crumbs": [
      "Case-control studies"
    ]
  },
  {
    "objectID": "point_trts.html",
    "href": "point_trts.html",
    "title": "Weighting and standardisation for point treatments",
    "section": "",
    "text": "Data\nIn this practical we will use the ‘rotterdam’ data set, which includes data on individuals who underwent surgery for primary breast cancer between 1978 and 1993, and whose data were recorded in the Rotterdam Tumour Bank. The data include information on treatments received alongside a number of individual characteristics. Individuals were followed up for disease recurrence and death for up to a maximum of 19.3 years. This data set is available as part of the ‘survival’ package in R, and it has been widely used to illustrate survival analysis methods [e.g. see Royston P, Altman D. External validation of a Cox prognostic model: principles and methods. BMC Medical Research Methodology 2013, 13:33].\nWe have provided a slightly modified version of the Rotterdam data set in which individual follow-up is recorded in years instead of days, and where we have applied censoring at 10 years. We have also created an additional variable ‘enodes’ which is a transformation of the nodes variable - this transformation has been used in several previous analyses of these data. Some individuals in the original data set have been excluded, as they had recorded death times after they were censored for recurrence, resulting in a final sample size of 2939 individuals.",
    "crumbs": [
      "Weighting and standardisation for point treatments"
    ]
  },
  {
    "objectID": "point_trts.html#aims",
    "href": "point_trts.html#aims",
    "title": "Weighting and standardisation for point treatments",
    "section": "Aims",
    "text": "Aims\nThe aim is to estimate the effect of hormone therapy use on survival up to 10 years. More specifically we will estimate the population average (marginal) survival curves if everyone had received hormone therapy (hormon = 1) and if everyone had not received hormone therapy (hormon = 0). This will be done using the IPTW approach and the standardisation (g-formula) approach.",
    "crumbs": [
      "Weighting and standardisation for point treatments"
    ]
  },
  {
    "objectID": "point_trts.html#load-data-and-packages",
    "href": "point_trts.html#load-data-and-packages",
    "title": "Weighting and standardisation for point treatments",
    "section": "Load data and packages",
    "text": "Load data and packages\n\nlibrary(survival)\nlibrary(adjustedCurves)\nlibrary(riskRegression)\n\ndta = readRDS(file = \"data/dta.rds\")\ndta$hormon = as.factor(dta$hormon)",
    "crumbs": [
      "Weighting and standardisation for point treatments"
    ]
  },
  {
    "objectID": "point_trts.html#simple-analyses",
    "href": "point_trts.html#simple-analyses",
    "title": "Weighting and standardisation for point treatments",
    "section": "Simple analyses",
    "text": "Simple analyses\n\nObtain and plot Kaplan-Meier estimates of the survival curves for people who did and did not receive hormone therapy. Which group had better survival?\n\n\nkm = survfit(Surv(dtime,death)~hormon,data=dta)\n\nplot(km, xlab=\"Time (years)\",ylab=\"Survival probability\",\n     col=c(\"red\",\"blue\"),lwd=2,conf.int=T,main=\"\")\nlegend(x=\"bottomleft\",c(\"Treatment: No\",\"Treatment: Yes\"),\n       col=c(\"red\",\"blue\"),lty=1,lwd=2,bty=\"n\")\n\n\n\n\n\n\n\n\n\nUsing coxph, fit the following Cox regression models: (a) an unadjusted model including ‘hormon’ only, (b) an adjusted model including ‘hormon’ and the following potential confounders: age, meno, size, grade, enodes, pgr, er, chemo. Compare the estimated hazard ratios for ‘hormon’ in the unadjusted and adjusted models.\n\n\ncox.unadj = coxph(Surv(dtime,death)~hormon,data=dta)\nsummary(cox.unadj)\n\nCall:\ncoxph(formula = Surv(dtime, death) ~ hormon, data = dta)\n\n  n= 2939, number of events= 1139 \n\n           coef exp(coef) se(coef)     z Pr(&gt;|z|)    \nhormon1 0.41649   1.51662  0.08748 4.761 1.93e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n        exp(coef) exp(-coef) lower .95 upper .95\nhormon1     1.517     0.6594     1.278       1.8\n\nConcordance= 0.52  (se = 0.005 )\nLikelihood ratio test= 20.5  on 1 df,   p=6e-06\nWald test            = 22.67  on 1 df,   p=2e-06\nScore (logrank) test = 22.99  on 1 df,   p=2e-06\n\ncox.adj = coxph(Surv(dtime,death)~hormon+age+meno+size+as.factor(grade)+enodes+\n                 pgr+er+chemo,data=dta)\nsummary(cox.adj)\n\nCall:\ncoxph(formula = Surv(dtime, death) ~ hormon + age + meno + size + \n    as.factor(grade) + enodes + pgr + er + chemo, data = dta)\n\n  n= 2939, number of events= 1139 \n\n                        coef  exp(coef)   se(coef)       z Pr(&gt;|z|)    \nhormon1           -0.2455480  0.7822757  0.0926336  -2.651  0.00803 ** \nage                0.0088660  1.0089054  0.0040484   2.190  0.02852 *  \nmeno               0.0414142  1.0422837  0.1060775   0.390  0.69623    \nsize20-50          0.3944697  1.4835973  0.0705760   5.589 2.28e-08 ***\nsize&gt;50            0.6695558  1.9533695  0.0983801   6.806 1.00e-11 ***\nas.factor(grade)3  0.3225828  1.3806893  0.0762842   4.229 2.35e-05 ***\nenodes            -1.8736260  0.1535658  0.1098238 -17.060  &lt; 2e-16 ***\npgr               -0.0005765  0.9994236  0.0001445  -3.989 6.64e-05 ***\ner                -0.0001394  0.9998606  0.0001226  -1.137  0.25555    \nchemo             -0.0993596  0.9054171  0.0861474  -1.153  0.24876    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                  exp(coef) exp(-coef) lower .95 upper .95\nhormon1              0.7823     1.2783    0.6524    0.9380\nage                  1.0089     0.9912    1.0009    1.0169\nmeno                 1.0423     0.9594    0.8466    1.2832\nsize20-50            1.4836     0.6740    1.2919    1.7037\nsize&gt;50              1.9534     0.5119    1.6108    2.3688\nas.factor(grade)3    1.3807     0.7243    1.1889    1.6034\nenodes               0.1536     6.5119    0.1238    0.1904\npgr                  0.9994     1.0006    0.9991    0.9997\ner                   0.9999     1.0001    0.9996    1.0001\nchemo                0.9054     1.1045    0.7648    1.0720\n\nConcordance= 0.707  (se = 0.008 )\nLikelihood ratio test= 600.7  on 10 df,   p=&lt;2e-16\nWald test            = 651  on 10 df,   p=&lt;2e-16\nScore (logrank) test = 743.1  on 10 df,   p=&lt;2e-16",
    "crumbs": [
      "Weighting and standardisation for point treatments"
    ]
  },
  {
    "objectID": "point_trts.html#estimating-marginal-survival-curves-using-iptw",
    "href": "point_trts.html#estimating-marginal-survival-curves-using-iptw",
    "title": "Weighting and standardisation for point treatments",
    "section": "Estimating marginal survival curves using IPTW",
    "text": "Estimating marginal survival curves using IPTW\n\nWe will start by estimating the weights.\n\n\nFit a logistic regression model for treatment (hormon) conditional on the potential confounders, using the same set of variables as used in the adjusted Cox model in Part A, question 2.\nUse the previous model to obtain estimated inverse probability of treatment weights \\[\nW=\\frac{A}{\\mathbb{P}(A=1|L)}+\\frac{(1-A)}{\\mathbb{P}(A=0|L)}.\n\\] Take a look at the distribution of the weights.\n\n\n#Fit the model for treatment\nmod.treat = glm(hormon~age+meno+size+as.factor(grade)+enodes+pgr+er+chemo,\n               data=dta,family=\"binomial\")\nsummary(mod.treat)\n\n\nCall:\nglm(formula = hormon ~ age + meno + size + as.factor(grade) + \n    enodes + pgr + er + chemo, family = \"binomial\", data = dta)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -1.6858529  0.4768463  -3.535 0.000407 ***\nage                0.0108156  0.0080866   1.337 0.181072    \nmeno               1.3907115  0.2484181   5.598 2.17e-08 ***\nsize20-50          0.0713970  0.1487264   0.480 0.631188    \nsize&gt;50           -0.0657177  0.2106971  -0.312 0.755112    \nas.factor(grade)3  0.3358180  0.1658193   2.025 0.042846 *  \nenodes            -2.9076117  0.2306816 -12.604  &lt; 2e-16 ***\npgr               -0.0003873  0.0003010  -1.286 0.198296    \ner                -0.0004711  0.0002733  -1.724 0.084717 .  \nchemo             -0.6691812  0.2410921  -2.776 0.005510 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2081.2  on 2938  degrees of freedom\nResidual deviance: 1672.5  on 2929  degrees of freedom\nAIC: 1692.5\n\nNumber of Fisher Scoring iterations: 6\n\n#predicted probability of treatment from the model for each individual\npred.treat = predict(mod.treat,data=dta,type=\"response\")\n\n#Obtain the weight for each person\ndta$wt = (dta$hormon==1)/pred.treat+(dta$hormon==0)/(1-pred.treat)\n\n#take a look at the distribution of the weights\nhist(dta$wt,breaks=50)\n\n\n\n\n\n\n\n\n\nObtain estimates of the marginal survival curves under the two treatment strategies using a weighted Kaplan-Meier analysis (using survfit with the weights option), and plot these.\n\n\n#Weighted Kaplan-Meier analysis\n\nkm.wt = survfit(Surv(dtime,death)~hormon,data=dta,weights=dta$wt)\n\n#plot the estimated marginal survival curves\nplot(km.wt, xlab=\"Time (years)\",ylab=\"Survival probability\",\n     col=c(\"red\",\"blue\"),lwd=2,conf.int=F,main=\"\")\nlegend(x=\"bottomleft\",c(\"Treatment: No\",\"Treatment: Yes\"),\n       col=c(\"red\",\"blue\"),lty=1,lwd=2,bty=\"n\")\n\n\n\n\n\n\n\n\n\nFit a weighted Cox regression (using coxph with the weights option) including ‘hormon’ only in the model. Use the results from the model to obtain estimates of the marginal survival curves under the two treatment strategies (using survfit), and plot your estimated curves.\n\n\n#Using Weighted Cox regression\n\ncox.wt = coxph(Surv(dtime,death)~hormon,data=dta,weights=dta$wt)\nsummary(cox.wt)\n\nCall:\ncoxph(formula = Surv(dtime, death) ~ hormon, data = dta, weights = dta$wt)\n\n  n= 2939, number of events= 1139 \n\n            coef exp(coef) se(coef) robust se      z Pr(&gt;|z|)  \nhormon1 -0.38362   0.68139  0.04537   0.17176 -2.233   0.0255 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n        exp(coef) exp(-coef) lower .95 upper .95\nhormon1    0.6814      1.468    0.4866    0.9541\n\nConcordance= 0.549  (se = 0.021 )\nLikelihood ratio test= 72.81  on 1 df,   p=&lt;2e-16\nWald test            = 4.99  on 1 df,   p=0.03\nScore (logrank) test = 72.38  on 1 df,   p=&lt;2e-16,   Robust = 5.7  p=0.02\n\n  (Note: the likelihood ratio and score tests assume independence of\n     observations within a cluster, the Wald and robust score tests do not).\n\n#plot estimated marginal survival curves\n# censor = F means: include only event time points\nsurv.1 = survfit(cox.wt,newdata=data.frame(hormon=factor(1)),censor = F)$surv\nsurv.0 = survfit(cox.wt,newdata=data.frame(hormon=factor(0)),censor = F)$surv\n\ntimes = sort(unique(dta$dtime[dta$death==1]))\nplot(times,surv.1,type=\"s\",col=\"blue\",lwd=2,ylim=c(0,1),\n     xlab=\"Time (years)\",ylab=\"Survival probability\")\nlines(times,surv.0,type=\"s\",col=\"red\",lwd=2)\nlegend(x=\"bottomleft\",c(\"Treatment: No\",\"Treatment: Yes\"),\n       col=c(\"red\",\"blue\"),lwd=2,bty=\"n\")\n\n\n\n\n\n\n\n\n\nBased on your Kaplan-Meier and Cox regression analyses in questions 2 and 3, obtain estimates of the marginal risk of death up to time 5 under the two treatment strategies, and the corresponding risk difference.\n\n\n#---\n#Using Weighted Kaplan-Meier\n\nsummary(km.wt,times=5)\n\nCall: survfit(formula = Surv(dtime, death) ~ hormon, data = dta, weights = dta$wt)\n\n                hormon=0 \n        time       n.risk      n.event     survival      std.err lower 95% CI \n    5.00e+00     2.05e+03     7.83e+02     7.31e-01     9.26e-03     7.13e-01 \nupper 95% CI \n    7.50e-01 \n\n                hormon=1 \n        time       n.risk      n.event     survival      std.err lower 95% CI \n    5.00e+00     2.12e+03     5.73e+02     7.97e-01     3.26e-02     7.35e-01 \nupper 95% CI \n    8.63e-01 \n\n1-summary(km.wt,times=5)$surv\n\n[1] 0.2688313 0.2034598\n\n#Risk difference\n(1-summary(km.wt,times=5)$surv[2])-(1-summary(km.wt,times=5)$surv[1])\n\n[1] -0.06537144\n\n#---\n#Using Weighted Cox regression\n\nsurv.1.t5 = summary(survfit(cox.wt,newdata=data.frame(hormon=factor(1)),censor = F),times=5)\n\nsurv.0.t5 = summary(survfit(cox.wt,newdata=data.frame(hormon=factor(0)),censor = F),times=5)\n\n#Risk difference\n(1-surv.1.t5$surv)-(1-surv.0.t5$surv)\n\n[1] -0.07827857\n\n\n\nEXTRA: Use the adjustedsurv function in the adjustedCurves package to obtain the marginal survival curves and the marginal risks and risk differences at time 5. Use the option method=iptw_km.\n\n\n#marginal survival curves\nadjsurv.wt  =  adjustedsurv(data=dta,variable=\"hormon\",ev_time=\"dtime\",event=\"death\",\n                        method=\"iptw_km\",\n                        treatment_model=mod.treat,\n                        conf_int=TRUE,\n                        bootstrap=F)\n\nplot(adjsurv.wt,conf_int=T,custom_colors=c(\"red\",\"blue\"),xlab=\"Time\",legend.title=\"\",ylim=c(0,1))\n\nLoading required namespace: pammtools\n\n\n\n\n\n\n\n\n#survival probabilities at time 5\nadjsurv.wt  =  adjustedsurv(data=dta,variable=\"hormon\",ev_time=\"dtime\",event=\"death\",\n                           method=\"iptw_km\",\n                           treatment_model=mod.treat,\n                           times=5,\n                           conf_int=TRUE,\n                           bootstrap=F)\n\nadjsurv.wt$adjsurv\n\nNULL",
    "crumbs": [
      "Weighting and standardisation for point treatments"
    ]
  },
  {
    "objectID": "point_trts.html#estimating-marginal-survival-curves-using-standardisation-g-formula",
    "href": "point_trts.html#estimating-marginal-survival-curves-using-standardisation-g-formula",
    "title": "Weighting and standardisation for point treatments",
    "section": "Estimating marginal survival curves using standardisation (g-formula)",
    "text": "Estimating marginal survival curves using standardisation (g-formula)\n\nStart by fitting an adjusted Cox regression including both treatment (`hormon’) and the set of potential confounders, as in Part A, question 2.\nWe will now perform the standardisation based on the adjusted Cox model.\n\n\nCreate a new data set which is the same as dta but where you set hormon=1 for everyone. Their other variables remain the same. Call this dta.1.\nUsing the Cox model from question 1, use survfit to obtain the estimated survival probability for each individual in dta.1 at all observed event or censoring times times. This gives a matrix of estimated survival probabilities for each individual at each time under the treatment strategy of setting hormon=1.\nCalculate the average survival probability at each time, i.e. averaging over individuals at each time.\n\n\n#create dataset in which treatment is set to 1 for everyone\ndta.1 = dta\ndta.1$hormon = 1\ndta.1$hormon = as.factor(dta.1$hormon)\n\n#predicted survival probabilities at all observed event times \n# for each individual under strategy hormon=1\nsurv.1 = survfit(cox.adj,newdata=dta.1,censor = F)\n\n#mean survival probability at all times\n#averaging over all individuals under strategy hormon=1\nsurvmean.1 = rowMeans(surv.1$surv)\n\n\nRepeat question 2 but setting hormon=0 for everyone.\n\n\n#create dataset in which treatment is set to 0 for everyone\ndta.0 = dta\ndta.0$hormon = 0\ndta.0$hormon = as.factor(dta.0$hormon)\n\n#predicted survival probabilities at all observed event times \n# for each individual under strategy hormon=0\nsurv.0 = survfit(cox.adj,newdata=dta.0,censor = F)\n\n#mean survival probability at all times\n#averaging over all individuals under strategy hormon=0\nsurvmean.0 = rowMeans(surv.0$surv)\n\n\nPlot the marginal survival curves using your results from questions 2 and 3.\n\n\ntimes = surv.1$time\nplot(times,survmean.1,type=\"s\",col=\"blue\",lwd=2,ylim=c(0,1),\n     xlab=\"Time (years)\",ylab=\"Survival probability\")\nlines(times,survmean.0,type=\"s\",col=\"red\",lwd=2)\nlegend(x=\"bottomleft\",c(\"Treatment: No\",\"Treatment: Yes\"),\n       col=c(\"red\",\"blue\"),lwd=2,bty=\"n\")\n\n\n\n\n\n\n\n\n\nObtain estimates of the marginal risk of death up to time 5 under the two treatment strategies, and the corresponding risk difference.\n\n\n#predicted survival probabilities at time 5 for each individual under strategy hormon=1\nsurv.1.t5 = summary(surv.1,times=5)\n\n#predicted survival probabilities at time 5 for each individual under strategy hormon=0\nsurv.0.t5 = summary(surv.0,times=5)\n\n#mean survival probability at time 5, averaging over all individuals\n#under strategy hormon=1 and under strategy hormon=0\nsurvmean.1.t5 = mean(surv.1.t5$surv)\nsurvmean.0.t5 = mean(surv.0.t5$surv)\n\n#Risk difference at time 5\nrd.t5 = survmean.1.t5-survmean.0.t5\n\n\nUse the adjustedsurv function in the ‘adjustedCurves’ package to obtain the marginal survival curves and the marginal risks and risk differences. Use the option method=direct.\n\n\n#marginal survival curves\n\n#We need to use the 'x=TRUE' option in coxph\ncox.adj = coxph(Surv(dtime,death)~hormon+age+meno+size+as.factor(grade)+enodes+\n                 pgr+er+chemo,data=dta,x=TRUE)\n\n\nadjsurv.gform  =  adjustedsurv(data=dta,variable=\"hormon\",ev_time=\"dtime\",event=\"death\",\n                        method=\"direct\",\n                        outcome_model=cox.adj,\n                        conf_int=TRUE,\n                        bootstrap=F)\n\nplot(adjsurv.gform,conf_int=T,custom_colors=c(\"red\",\"blue\"),\n     xlab=\"Time (years)\",legend.title=\"\",ylim=c(0,1))\n\n\n\n\n\n\n\n#survival and risk difference probabilities at time 5\n\nadjsurv  =  adjustedsurv(data=dta,variable=\"hormon\",ev_time=\"dtime\",event=\"death\",\n                        method=\"direct\",\n                        outcome_model=cox.adj,\n                        conf_int=TRUE,\n                        bootstrap=F,\n                        times=5)\n\n#risk diff at time 5\nadjsurv$ate_object$diffRisk\n\n   estimator  time      A estimate.A      B estimate.B    estimate         se\n      &lt;char&gt; &lt;num&gt; &lt;char&gt;      &lt;num&gt; &lt;char&gt;      &lt;num&gt;       &lt;num&gt;      &lt;num&gt;\n1:  GFORMULA     5      0  0.2670785      1  0.2204934 -0.04658518 0.01568172\n         lower       upper     p.value\n         &lt;num&gt;       &lt;num&gt;       &lt;num&gt;\n1: -0.07732079 -0.01584957 0.002971533",
    "crumbs": [
      "Weighting and standardisation for point treatments"
    ]
  },
  {
    "objectID": "time_dep_trts.html",
    "href": "time_dep_trts.html",
    "title": "Censoring and time-dependent treatment strategies",
    "section": "",
    "text": "Data\nIn this practical we will use a simulated data set, which includes data on 1000 individuals. The data include information on time-dependent treatment status \\(A\\), alongside three confounding variables (\\(X,L_1,L_2\\)), two of which are time-dependent. Individuals were followed up for death for up to 3 years.\nYou can assume the relationships between the variables is as depicted in the discrete time DAG below, where \\(Y_t\\) denotes the event indicator \\(D\\) at time \\(t\\) (\\(t=1,2,3\\)).",
    "crumbs": [
      "Censoring and time-dependent treatment strategies"
    ]
  },
  {
    "objectID": "time_dep_trts.html#aims",
    "href": "time_dep_trts.html#aims",
    "title": "Censoring and time-dependent treatment strategies",
    "section": "Aims",
    "text": "Aims\nThis practical exercise is in two parts.\nIn Part A the aim is to estimate the effect of treatment status at time 0 (\\(A_0\\)) on survival up to 3 years. This part ignores that treatment status changes over time, and it can be considered as a type of intention-to-treat (ITT) analysis. In this part we will also explore how to handle dependent censoring using inverse probability of censoring weights (IPCW).\nIn Part B the aim is to estimate the effect of sustained use of the treatment vs sustained non-use of the treatment on survival up to 3 years. We will use the cloning-censoring-weighting approach to estimate the population average (marginal) survival curves if everyone had received treatment \\(A\\) from time 0 onwards (\\(a_0=a_1=a_2=1\\)) and if everyone had not received treatment \\(A\\) from time 0 onwards (\\(a_0=a_1=a_2=0\\)).",
    "crumbs": [
      "Censoring and time-dependent treatment strategies"
    ]
  },
  {
    "objectID": "time_dep_trts.html#load-data-and-packages",
    "href": "time_dep_trts.html#load-data-and-packages",
    "title": "Censoring and time-dependent treatment strategies",
    "section": "Load data and packages",
    "text": "Load data and packages\nIn this practical we will use the following packages: survival, tidyverse, splines.\n\nlibrary(survival)\nlibrary(tidyverse)\nlibrary(splines)\n\ndta = readRDS(file = \"data/dta_long.rds\")",
    "crumbs": [
      "Censoring and time-dependent treatment strategies"
    ]
  },
  {
    "objectID": "time_dep_trts.html#impact-of-baseline-treatment-status-and-using-ipcw",
    "href": "time_dep_trts.html#impact-of-baseline-treatment-status-and-using-ipcw",
    "title": "Censoring and time-dependent treatment strategies",
    "section": "Impact of baseline treatment status and using IPCW",
    "text": "Impact of baseline treatment status and using IPCW\n\nCheck out the format of the data. How many rows of data are there? How many events?\n\n\n#data for the first 5 individuals\ndta[dta$id%in%1:5,]\n\n    id visit T.start    T.stop D A           X          L1 L2\n1.0  1     0       0 0.1158240 0 0 -0.06264538 -0.82276938  1\n2.0  2     0       0 1.0000000 0 0  0.01836433 -1.79637025  0\n2.1  2     1       1 2.0000000 0 0  0.01836433 -0.44665445  1\n2.2  2     2       2 2.6234834 0 1  0.01836433 -0.34086086  1\n3.0  3     0       0 1.0000000 0 1 -0.08356286  1.46577269  1\n3.1  3     1       1 2.0000000 0 1 -0.08356286  1.48940024  1\n3.2  3     2       2 3.0000000 0 0 -0.08356286 -1.13274309  0\n4.0  4     0       0 0.4539474 0 1  0.15952808  0.66796553  1\n5.0  5     0       0 0.4846428 0 0  0.03295078 -0.02254975  1\n\ndim(dta)\n\n[1] 1827    9\n\ntable(dta$D)\n\n\n   0    1 \n1640  187 \n\n\n\nGenerate the baseline values of \\(A,L_1,L_2\\) (i.e. the values at time 0) so that the baseline values are stored in each row of data for a given individual. We will refer to these as \\(A_0, L_{10}, L_{20}\\).\n\n\ndta$A0 = ave(dta$A, dta$id, FUN=function(x){x[1]})\ndta$L10 = ave(dta$L1, dta$id, FUN=function(x){x[1]})\ndta$L20 = ave(dta$L2, dta$id, FUN=function(x){x[1]})\n\nhead(dta)\n\n    id visit T.start   T.stop D A           X         L1 L2 A0        L10 L20\n1.0  1     0       0 0.115824 0 0 -0.06264538 -0.8227694  1  0 -0.8227694   1\n2.0  2     0       0 1.000000 0 0  0.01836433 -1.7963702  0  0 -1.7963702   0\n2.1  2     1       1 2.000000 0 0  0.01836433 -0.4466544  1  0 -1.7963702   0\n2.2  2     2       2 2.623483 0 1  0.01836433 -0.3408609  1  0 -1.7963702   0\n3.0  3     0       0 1.000000 0 1 -0.08356286  1.4657727  1  1  1.4657727   1\n3.1  3     1       1 2.000000 0 1 -0.08356286  1.4894002  1  1  1.4657727   1\n\n\n\nPerform an unweighted (i.e. unadjusted) Kaplan-Meier analysis by treatment status at baseline, \\(A_0\\), and plot the estimated survival curves. Here we need to take account of the long format of the data using Surv(T.start,T.stop,D) in survfit. What is the crude association between \\(A_0\\) and survival? Obtain the estimated survival probabilities at time 3.\n\n\nkm.unwt = survfit(Surv(T.start,T.stop,D)~A0,data=dta)\n\nplot(km.unwt, xlab=\"Time\",ylab=\"Survival probability\",\n     col=c(\"red\",\"blue\"),lwd=2,conf.int=F,main=\"\")\nlegend(x=\"bottomleft\",c(\"Treatment strategy A=0\",\"Treatment strategy A=1\"),\n       col=c(\"red\",\"blue\"),lty=1,lwd=2,bty=\"n\")\n\n\n\n\n\n\n\n#survival probabilities at time 3\nsummary(km.unwt,times=3)\n\nCall: survfit(formula = Surv(T.start, T.stop, D) ~ A0, data = dta)\n\n                A0=0 \n        time       n.risk      n.event     survival      std.err lower 95% CI \n      3.0000     112.0000     131.0000       0.6638       0.0268       0.6133 \nupper 95% CI \n      0.7184 \n\n                A0=1 \n        time       n.risk      n.event     survival      std.err lower 95% CI \n      3.0000      60.0000      56.0000       0.7153       0.0365       0.6472 \nupper 95% CI \n      0.7905 \n\n\n\nIn this question we will estimate marginal survival curves under the strategies of setting \\(A_0=1\\) for everyone and setting \\(A_0=0\\) for everyone, denoted \\(S^{a_0=1}(t)=\\mathbb{P}(T^{a_0=1}&gt;t)\\) and \\(S^{a_0=0}(t)=\\mathbb{P}(T^{a_0=0}&gt;t)\\).\n\n\nUsing a logistic regression model, generate inverse probability of treatment weights of the form \\[\niptw=\\frac{A_0}{\\mathbb{P}(A_{0}=1|X,L_{10},L_{20})}+\\frac{(1-A_0)}{\\mathbb{P}(A_{0}=0|X,L_{10},L_{20})}\n\\] The model used to generate the weights should be fitted using only the first row of data for each individual.\nPerform a weighted Kaplan-Meier analysis by treatment status at baseline (\\(A_0\\)) using the weights generated in (a).\nPlot and interpret the estimated survival curves under the two treatment strategies. Obtain estimates of \\(S^{a_0=1}(t)\\) and \\(S^{a_0=0}(t)\\) for \\(t=3\\).\n\n\n#Fit the model for treatment at baseline\nmod.treat = glm(A0~X+L10+L20,\n               data=dta[dta$T.start==0,],family=\"binomial\")\n\n#predicted probability of treatment from the model for each individual\npred.treat = predict(mod.treat,newdata=dta,type=\"response\")\n\n#Obtain the weight for each person\ndta$iptw = (dta$A0==1)/pred.treat+(dta$A0==0)/(1-pred.treat)\n\n#Weighted Kaplan-Meier\nkm.wt = survfit(Surv(T.start,T.stop,D)~A0,data=dta,weights = dta$iptw)\n\nplot(km.wt, xlab=\"Time\",ylab=\"Survival probability\",\n     col=c(\"red\",\"blue\"),lwd=2,conf.int=F,main=\"\")\nlegend(x=\"bottomleft\",c(\"Treatment strategy A0=0\",\"Treatment strategy A0=1\"),\n       col=c(\"red\",\"blue\"),lty=1,lwd=2,bty=\"n\")\n\n\n\n\n\n\n\n#survival probabilities at time 3\nsummary(km.wt,times=3)\n\nCall: survfit(formula = Surv(T.start, T.stop, D) ~ A0, data = dta, \n    weights = dta$iptw)\n\n                A0=0 \n        time       n.risk      n.event     survival      std.err lower 95% CI \n      3.0000     148.2815     250.3064       0.5926       0.0307       0.5355 \nupper 95% CI \n      0.6558 \n\n                A0=1 \n        time       n.risk      n.event     survival      std.err lower 95% CI \n      3.0000     263.4772     125.6947       0.8025       0.0342       0.7382 \nupper 95% CI \n      0.8725 \n\n\n\nThe analysis in question 3 assumes that any censoring in the data is independent of any variables that are also related to the outcome. It turns out that the censoring depends on \\(X\\) and \\(L_2\\), which are also related to the outcome (we know this because we generated the data!). In this question we will account for dependent censoring using inverse probability of censoring weights (IPCW) to estimate \\(S^{a_0=1,\\bar{c}_t=0}(t)=\\mathbb{P}(T^{a_0=1,\\bar{c}_t=0}&gt;t)\\) and \\(S^{a_0=0,\\bar{c}_t=0}(t)=\\mathbb{P}(T^{a_0=0,\\bar{c}_t=0}&gt;t)\\).\n\n\nGenerate an indicator for non-administrative censoring in each row for each individual. This should indicate whether the individual is (non-administratively) censored at time T.stop.\nFit a logistic regression model with your censoring indicator as the outcome and with \\(A, X,L_1,L_2\\) as covariates. Include visit as a factor variable.\nUse the model in (b) to estimate the inverse probability of censoring weight in each row: \\[\nipcw(t)=\\prod_{k=1}^{t}\\frac{1}{\\mathbb{P}(C_k=0|\\bar C_{k-1}=0,\\bar{A}_k,X,\\bar{L}_{1k},\\bar{L}_{2k})}, \\quad t=1,2,3\n\\]\nGenerate a combined weight by multiplying the IPTW by the IPCW, and perform a weighted Kaplan-Meier analysis by treatment status at baseline (\\(A_0\\)).\nPlot and interpret the estimated survival curves under the two treatment strategies. Obtain estimates of \\(S^{a_0=1,\\bar{c}_t=0}(t)\\) and \\(S^{a_0=1,\\bar{c}_t=0}(t)\\) for \\(t=3\\).\n\n\n# Create event variable for censoring\ndta$censored = ave(dta$D, dta$id, FUN = function(x){\n  ifelse(cumsum(x)==rep(0, length(x)), c(rep(0, length(x)-1), 1), rep(0, length(x)))})\ndta$censored = ifelse(dta$T.stop==3 & dta$D==0,0,dta$censored) \n\n# Fit pooled logistic regressions with censoring as outcome:\ncwfit.denum = glm(censored ~ as.factor(T.start) + A + X + L1 + L2, \n                   family=binomial, data=dta)\n\n# Predict individual censoring probabilities over time:\ncprob.denum = predict(cwfit.denum, dta, type=\"response\")\n\n# Calculate IPCW's:\ndta$ipcw = 1/(1-cprob.denum)\ndta$ipcw = ave(dta$ipcw, dta$id, FUN = cumprod)\n\n#combined weights\ndta$comb.wt = pmin(dta$ipcw*dta$iptw,50)\ndta$comb.wt = pmin(dta$ipcw*dta$iptw,50)\n\n#Weighted Kaplan-Meier\nkm.wt2 = survfit(Surv(T.start,T.stop,D)~A0,data=dta,weights = dta$comb.wt)\n\nplot(km.wt2, xlab=\"Time\",ylab=\"Survival probability\",\n     col=c(\"red\",\"blue\"),lwd=2,conf.int=F,main=\"\")\nlegend(x=\"bottomleft\",c(\"Treatment strategy A0=0\",\"Treatment strategy A0=1\"),\n       col=c(\"red\",\"blue\"),lty=1,lwd=2,bty=\"n\")\n\n\n\n\n\n\n\n#survival probabilities at time 3\nsummary(km.wt2,times=3)\n\nCall: survfit(formula = Surv(T.start, T.stop, D) ~ A0, data = dta, \n    weights = dta$comb.wt)\n\n                A0=0 \n        time       n.risk      n.event     survival      std.err lower 95% CI \n      3.0000     482.1156     631.2879       0.5132       0.0439       0.4340 \nupper 95% CI \n      0.6068 \n\n                A0=1 \n        time       n.risk      n.event     survival      std.err lower 95% CI \n      3.0000     565.6242     338.5842       0.7377       0.0455       0.6538 \nupper 95% CI \n      0.8324 \n\n\n\nEXTRA. The analysis in question 4 assumes that censoring only occurs at time 1, 2, or 3. The censoring times can actually be at any time (in this data set). One approach to allowing for this is to extend the analysis in 4 to allow censoring times to occur on a small grid of times, say \\(0.1,0.2,\\ldots,2.9,3.0\\). To allow this the first step is to divide each individual’s follow-up into smaller time periods according to the grid \\(0.1,0.2,\\ldots,2.9,3.0\\), which can be done using the survSplit function: dta.split = survSplit(Surv(T.start,T.stop,D)~.,data=dta,cut=seq(0,3,0.1)). The rest of the analysis then follows as above, except that in the censoring weights model we recommend including a spline term for T.start instead of the separate indicator for each time (e.g. ns(T.start,df=4)).\n\n\ndta.split = survSplit(Surv(T.start,T.stop,D)~.,data=dta,cut=seq(0,3,0.1))\n\n# Create new event variable for censoring\ndta.split$censored  =  ave(dta.split$D, dta.split$id, FUN = function(x){\n  ifelse(cumsum(x)==rep(0, length(x)), c(rep(0, length(x)-1), 1), rep(0, length(x)))})\ndta.split$censored  = ifelse(dta.split$T.stop==3 & dta.split$D==0,0,dta.split$censored) \n\n# Fit pooled logistic regressions with censoring as outcome\ncwfit.denum = glm(censored ~ splines::ns(T.start,df=4) + A + X + L1 + L2, \n                   family=binomial, data=dta.split)\n\n# Predict individual censoring probabilities over time:\ncprob.denum = predict(cwfit.denum, dta.split, type=\"response\")\n\n# Calculate IPCW's\ndta.split$ipcw = 1/(1-cprob.denum)\ndta.split$ipcw = ave(dta.split$ipcw, dta.split$id, FUN = cumprod)\n\n#combined weights\ndta.split$comb.wt = dta.split$ipcw*dta.split$iptw\n\n#Weighted Kaplan-Meier\nkm.wt3 = survfit(Surv(T.start,T.stop,D)~A0,data=dta.split,weights = dta.split$comb.wt)\n\nplot(km.wt3, xlab=\"Time\",ylab=\"Survival probability\",\n     col=c(\"red\",\"blue\"),lwd=2,conf.int=F,main=\"\")\nlegend(x=\"bottomleft\",c(\"Treatment strategy A0=0\",\"Treatment strategy A0=1\"),\n       col=c(\"red\",\"blue\"),lty=1,lwd=2,bty=\"n\")\n\n\n\n\n\n\n\n#survival probabilities at time 3\nsummary(km.wt3,times=3)\n\nCall: survfit(formula = Surv(T.start, T.stop, D) ~ A0, data = dta.split, \n    weights = dta.split$comb.wt)\n\n                A0=0 \n        time       n.risk      n.event     survival      std.err lower 95% CI \n       3.000      625.308      554.926        0.482        0.058        0.380 \nupper 95% CI \n       0.610 \n\n                A0=1 \n        time       n.risk      n.event     survival      std.err lower 95% CI \n       3.000      640.274      283.014        0.727        0.049        0.637 \nupper 95% CI \n       0.830",
    "crumbs": [
      "Censoring and time-dependent treatment strategies"
    ]
  },
  {
    "objectID": "time_dep_trts.html#sustained-treatment-strategies-and-the-cloning-censoring-weighting-approach",
    "href": "time_dep_trts.html#sustained-treatment-strategies-and-the-cloning-censoring-weighting-approach",
    "title": "Censoring and time-dependent treatment strategies",
    "section": "Sustained treatment strategies and the cloning-censoring-weighting approach",
    "text": "Sustained treatment strategies and the cloning-censoring-weighting approach\nThe aim in this part is to estimate survival curves if everyone had received treatment \\(A\\) from time 0 onwards (\\(a_0=a_1=a_2=1\\)) and if everyone had not received treatment \\(A\\) from time 0 onwards (\\(a_0=a_1=a_2=0\\)). The estimands are denoted \\(S^{\\underline{a}_0=1}(t)=\\mathbb{P}(T^{\\underline{a}_0=1}&gt;t)\\) and \\(S^{\\underline{a}_0=0}(t)=\\mathbb{P}(T^{\\underline{a}_0=0}&gt;t)\\). The questions below take us through the steps required to perform the cloning-censoring-weighting analysis.\n\nIn this first question we will perform the cloning step and generate some variables needed for subsequent data manipulation and analysis.\n\n\nCreate two copies of the data, one corresponding to each of the two treatment strategies of interest. Denote these clone.1 (for strategy \\(a_0=a_1=a_2=1\\)) and clone.0 (for strategy \\(a_0=a_1=a_2=0\\)).\nIn the clone.1 data generate a new variable indicating whether a person has deviated from the strategy \\(a_0=a_1=a_2=1\\). Note that some individuals deviate from this strategy immediately because they have \\(A_0=0\\) in their first row. Keep all rows of data up to and including the deviation row in which the person first deviates from the strategy \\(a_0=a_1=a_2=1\\), and discard rows after that in which the person has first deviated. Later we will drop the deviation row, but we need to keep it for now in order to estimate the artificial censoring weights (question 2!).\nRepeat (b) for the clone.0 data, thinking now of the strategy \\(a_0=a_1=a_2=0\\).\n\n\n#Create two copies of the data\nclone.1 = dta\nclone.0 = dta\n\n#---\n#For clone.1\n\n#cumulative sum of treatment status being A=1\nclone.1$cumsumA = ave(clone.1$A==1,clone.1$id,FUN=cumsum)\n\n#indicator of whether the person has deviated from A=1\nclone.1$switchA = ifelse(clone.1$cumsumA==clone.1$T.start+1,0,1)\n\n#cumulative indicator of whether the person has deviated from A=1\nclone.1$switchA = ave(clone.1$switchA,clone.1$id,FUN=cumsum)\n\n#drop if switchA&gt;1 \n#because we are only going to use the rows where switchA=0 or 1 to estimate the weights.\nclone.1 = clone.1[clone.1$switchA&lt;=1,]\n\n#---\n#For clone.0\n\n#cumulative sum of treatment status being A=0\nclone.0$cumsumA = ave(clone.0$A==0,clone.0$id,FUN=cumsum)\n\n#indicator of whether the person has deviated from A=0\nclone.0$switchA = ifelse(clone.0$cumsumA==clone.0$T.start+1,0,1)\n\n#cumulative indicator of whether the person has deviated from A=0\nclone.0$switchA = ave(clone.0$switchA,clone.0$id,FUN=cumsum)\n\n#drop if cumsum.switchA&gt;1 \n#because we are only going to use the rows where switchA=0 or 1 to estimate the weights.\nclone.0 = clone.0[clone.0$switchA&lt;=1,]\n\n\nIn question 1 we have generated a variable switchA which is switchA=0 in rows in which the individual is following the treatment strategy of interest, and which is switchA=1 in the first row in which the person has deviated from the strategy (rows after this have been excluded). Next we will use this variable to estimate the artificial censoring weights. Consider first the data set clone.1 and use the following steps:\n\n\nFit a logistic regression model with switchA as the outcome and with covariates the variables \\(X,L_{2},L_{1}\\) from the same row, plus an indicator for T.start. This model can be fitted using all rows combined.\nDrop the rows in which switchA=1. we are now finished with this variable.\nUse the model in (a) to obtain the artificial censoring weights. The weight in a given row is the probability that a person remains uncensored (i.e. that they have not switched from the treatment strategy of interest): \\[\nW(t)=\\prod_{k=0}^{t}\\frac{1}{\\mathbb{P}(switchA_k=0|X,L_{2k},L_{1k})},\\quad t=0,1,2\n\\]\nRepeat steps (a)-(c) for the data set clone.0.\n\n\n#weights models\n\nwt.mod.denom.1=glm(switchA~as.factor(T.start)+X+L1+L2,family=\"binomial\",data=clone.1)\nclone.1$wt.denom = 1-predict(wt.mod.denom.1,type = \"response\",newdata = clone.1)\n\nwt.mod.denom.0=glm(switchA~as.factor(T.start)+X+L1+L2,family=\"binomial\",data=clone.0)\nclone.0$wt.denom = 1-predict(wt.mod.denom.0,type = \"response\",newdata = clone.0)\n\n#drop the row where the switch occurred\n\nclone.1 = clone.1[clone.1$switchA==0,]\nclone.0 = clone.0[clone.0$switchA==0,]\n\n#ipcw for the artificial censoring\n\nclone.1$wt = 1/clone.1$wt.denom\nclone.1$wt = ave(clone.1$wt,clone.1$id,FUN=cumprod)\n\nclone.0$wt = 1/clone.0$wt.denom\nclone.0$wt = ave(clone.0$wt,clone.0$id,FUN=cumprod)\n\n\nFinally, we perform the weighting step of the cloning-censoring-weighting approach, by performing a weighted analysis using the weights generated in question 2.\n\n\nUsing the data set clone.1 perform a weighted Kaplan-Meier analysis to estimate \\[S^{\\underline{a}_0=1}(t)=\\mathbb{P}(T^{\\underline{a}_0=1}&gt;t)\\]\nUsing the data set clone.0 perform a weighted Kaplan-Meier analysis to estimate \\[S^{\\underline{a}_0=0}(t)=\\mathbb{P}(T^{\\underline{a}_0=0}&gt;t)\\]\nCompare the estimated survival curves in a plot and interpret the results.\nObtain the estimated survival probability at time 3 under each treatment strategy, and the corresponding risk difference\n\n\nkm.1 = survfit(Surv(T.start,T.stop,D)~1,data=clone.1,weights = clone.1$wt)\nkm.0 = survfit(Surv(T.start,T.stop,D)~1,data=clone.0,weights = clone.0$wt)\n\nplot(km.1$time,km.1$surv,type=\"s\",col=\"blue\",lwd=2,\n     xlab=\"Time\",ylab=\"Survival probability\",ylim=c(0,1))\nlines(km.0$time,km.0$surv,type=\"s\",col=\"red\",lwd=2)\nlegend(\"bottomleft\",c(\"Sustained A=1\",\"Sustained A=0\"),col=c(\"blue\",\"red\"),lwd=2)\n\n\n\n\n\n\n\n#survival probabilities and risk difference at time 3\nsummary(km.1,times=3)$surv\n\n[1] 0.8078718\n\nsummary(km.0,times=3)$surv\n\n[1] 0.4359687\n\n(1-summary(km.1,times=3)$surv)-(1-summary(km.0,times=3)$surv)\n\n[1] -0.3719031\n\n\n\nEXTRA. The above cloning-censoring-weighting analysis does not account for any standard censoring that may depend on covariates. This can be done by combining the censoring weights estimated in Part A, with the cloning-censoring-weighting approach performed in Part B. We recommend estimating the standard censoring weights first.\n\n\n#Demonstrated here by starting from dta.split, \n# which contains the ipcw for standard censoring.\n\n#Create two copies of the data\nclone.1 = dta.split\nclone.0 = dta.split\n\n#---\n#For clone.1\n\n#cumulative sum of treatment status being A=1\nclone.1$cumsumA = ave(clone.1$A==1,clone.1$id,FUN=cumsum)\n\n#indicator of whether the person has deviated from A=1\nclone.1$rownum=ave(rep(1,dim(clone.1)[1]),clone.1$id,FUN=cumsum)\nclone.1$switchA = ifelse(clone.1$cumsumA==clone.1$rownum,0,1)\n\n#cumulative indicator of whether the person has deviated from A=1\nclone.1$switchA = ave(clone.1$switchA,clone.1$id,FUN=cumsum)\n\n#drop if switchA&gt;1 \n#because we are only going to use the rows where switchA=0 or 1 to estimate the weights.\nclone.1 = clone.1[clone.1$switchA&lt;=1,]\n\n#---\n#For clone.0\n\n#cumulative sum of treatment status being A=0\nclone.0$cumsumA = ave(clone.0$A==0,clone.0$id,FUN=cumsum)\n\n#indicator of whether the person has deviated from A=0\nclone.0$rownum=ave(rep(1,dim(clone.0)[1]),clone.0$id,FUN=cumsum)\nclone.0$switchA = ifelse(clone.0$cumsumA==clone.0$rownum,0,1)\n\n#cumulative indicator of whether the person has deviated from A=0\nclone.0$switchA = ave(clone.0$switchA,clone.0$id,FUN=cumsum)\n\n#drop if cumsum.switchA&gt;1 \n#because we are only going to use the rows where switchA=0 or 1 to estimate the weights.\nclone.0 = clone.0[clone.0$switchA&lt;=1,]\n\n#weights models\n\nwt.mod.denom.1=glm(switchA~as.factor(visit)+X+L1+L2,family=\"binomial\",\n                   data=clone.1[clone.1$T.start%in%c(0,1,2),])\nclone.1$wt.denom = 1-predict(wt.mod.denom.1,type = \"response\",newdata = clone.1)\n\nwt.mod.denom.0=glm(switchA~as.factor(visit)+X+L1+L2,family=\"binomial\",\n                   data=clone.0[clone.0$T.start%in%c(0,1,2),])\nclone.0$wt.denom = 1-predict(wt.mod.denom.0,type = \"response\",newdata = clone.0)\n\n#drop the row where the switch occurred\n\nclone.1 = clone.1[clone.1$switchA==0,]\nclone.0 = clone.0[clone.0$switchA==0,]\n\n#ipcw for the artificial censoring\n\nclone.1$wt = ifelse(clone.1$T.start%in%c(0,1,2),1/clone.1$wt.denom,1)\nclone.1$wt = ave(clone.1$wt,clone.1$id,FUN=cumprod)\n\nclone.0$wt = ifelse(clone.0$T.start%in%c(0,1,2),1/clone.0$wt.denom,1)\nclone.0$wt = ave(clone.0$wt,clone.0$id,FUN=cumprod)\n\n#combine the artificial censoring weight with the standard censoring weight\nclone.1$comb.wt = clone.1$wt*clone.1$ipcw\nclone.0$comb.wt = clone.0$wt*clone.0$ipcw\n\n#weighted Kaplan-Meier analyses\n#Note that if we use wt instead of comb.wt then the results are \n# identical to those in question 4, as they should be\nkm.1 = survfit(Surv(T.start,T.stop,D)~1,data=clone.1,weights = clone.1$comb.wt)\nkm.0 = survfit(Surv(T.start,T.stop,D)~1,data=clone.0,weights = clone.0$comb.wt)\n\nplot(km.1$time,km.1$surv,type=\"s\",col=\"blue\",lwd=2,\n     xlab=\"Time\",ylab=\"Survival probability\",ylim=c(0,1))\nlines(km.0$time,km.0$surv,type=\"s\",col=\"red\",lwd=2)\nlegend(\"bottomleft\",c(\"Sustained A=1\",\"Sustained A=0\"),col=c(\"blue\",\"red\"),lwd=2)\n\n\n\n\n\n\n\n#survival probabilities and risk difference at time 3\nsummary(km.1,times=3)$surv\n\n[1] 0.7254016\n\nsummary(km.0,times=3)$surv\n\n[1] 0.2601855\n\n(1-summary(km.1,times=3)$surv)-(1-summary(km.0,times=3)$surv)\n\n[1] -0.4652161",
    "crumbs": [
      "Censoring and time-dependent treatment strategies"
    ]
  },
  {
    "objectID": "sustained_trts.html",
    "href": "sustained_trts.html",
    "title": "Marginal structural models and g-formula for sustained treatment strategies",
    "section": "",
    "text": "Data\nIn this practical we will use a simulated data set which includes data on 1000 individuals. The data include information on time-dependent treatment status \\(A\\), alongside three confounding variables (\\(X,L_1,L_2\\)), two of which are time-dependent. Individuals were followed up for death for up to 3 years.\nYou can assume the relationships between the variables is as depicted in the discrete time DAG below, where \\(Y_t\\) denotes the event indicator \\(D\\) at time \\(t\\) (\\(t=1,2,3\\)).",
    "crumbs": [
      "Marginal structural models and g-formula for sustained treatment strategies"
    ]
  },
  {
    "objectID": "sustained_trts.html#aims",
    "href": "sustained_trts.html#aims",
    "title": "Marginal structural models and g-formula for sustained treatment strategies",
    "section": "Aims",
    "text": "Aims\nThe aim is to estimate the effect of sustained use of the treatment vs sustained non-use of the treatment on survival up to 3 years. More specifically we will estimate the population average (marginal) survival curves if everyone had received treatment \\(A\\) from time 0 onwards (\\(a_0=a_1=a_2=1\\)) and if everyone had not received treatment \\(A\\) from time 0 onwards (\\(a_0=a_1=a_2=0\\)). The estimands are: \\[S^{\\underline{a}_0=1}(t)=\\mathbb{P}(T^{\\underline{a}_0=1}&gt;t)\\] \\[S^{\\underline{a}_0=0}(t)=\\mathbb{P}(T^{\\underline{a}_0=0}&gt;t)\\] Estimation will be performed using two methods:\n\nMarginal structural models estimated using IPTW\nG-formula",
    "crumbs": [
      "Marginal structural models and g-formula for sustained treatment strategies"
    ]
  },
  {
    "objectID": "sustained_trts.html#load-data-and-packages",
    "href": "sustained_trts.html#load-data-and-packages",
    "title": "Marginal structural models and g-formula for sustained treatment strategies",
    "section": "Load data and packages",
    "text": "Load data and packages\nIn this practical we will use the following packages: survival, tidyverse, data.table, gfoRmula:\n\nlibrary(survival)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(gfoRmula)\n\ndta = readRDS(file = \"data/dta_long.rds\")",
    "crumbs": [
      "Marginal structural models and g-formula for sustained treatment strategies"
    ]
  },
  {
    "objectID": "sustained_trts.html#marginal-structural-models-msms-estimated-using-iptw",
    "href": "sustained_trts.html#marginal-structural-models-msms-estimated-using-iptw",
    "title": "Marginal structural models and g-formula for sustained treatment strategies",
    "section": "Marginal structural models (MSMs) estimated using IPTW",
    "text": "Marginal structural models (MSMs) estimated using IPTW\nIn this part we will estimate the estimands of interest, \\(S^{\\underline{a}_0=1}(t)\\) and \\(S^{\\underline{a}_0=0}(t)\\), using MSMs estimated using time-dependent IPTW to handle the time-dependent confounding. We begin by setting up the data and estimating the weights, before using these to fit two MSMs for the hazard: \\[\nh^{\\underline{a}_0}(t)=h_0(t)e^{g(\\bar{a}_t;\\beta)}.\n\\]\n\nStart by generating lagged values of treatment \\(A\\) (denoted A_lag1 and A_lag2) and lagged values of the time-dependent covariates \\(L_1\\) (L1_lag1, L1_lag2) and \\(L_2\\) (L2_lag1, L2_lag2). These will be used later. The lag variables can be generated (for example) using the tidyverse package, e.g. dta = dta %&gt;% group_by(id) %&gt;% mutate(A_lag1=lag(A,1,default=0))\n\n\ndta = dta%&gt;%group_by(id)%&gt;%\n  mutate(A_lag1=lag(A,1,default=0),A_lag2=lag(A,2,default=0))%&gt;%\n  mutate(L1_lag1=lag(L1,1,default=0),L1_lag2=lag(L1,2,default=0))%&gt;%\n  mutate(L2_lag1=lag(L2,1,default=0),L2_lag2=lag(L2,2,default=0))\n\n\nIn this question we will estimate time-dependent inverse probability of treatment weights:\n\n\nUsing a logistic regression, fit a model for the probability of treatment at a given time, conditional on the current values of \\(L_1,L_2\\), treatment at the previous time point, baseline covariate \\(X\\), and an indicator for visit.\nUse the model to calculate the inverse probability of treatment weights at each time point. \\[\nW(t)=\\prod_{k=0}^{t}\\frac{1}{\\mathbb{P}(A_k|A_{k-1},X,L_{1k},L_{2k})}, \\quad t=0,1,2\n\\]\nCheck out the distribution of the weights\n\n\niptw.mod = glm(A~A_lag1+A_lag2+X+L1+L2+as.factor(visit),data=dta,family=\"binomial\")\n\npred.iptw = predict(iptw.mod,newdata=dta,type=\"response\")\n\ndta$iptw = dta$A/pred.iptw+(1-dta$A)/(1-pred.iptw)\n\ndta$iptw = ave(dta$iptw,dta$id,FUN=cumprod)\n\nhist(dta$iptw)\n\n\n\n\n\n\n\n\n\nThe weights estimated in question 1 are unstabilized. Obtain stabilized weights of the form below, and have a look at their distribution. \\[\nSW(t)=\\prod_{k=0}^{t}\\frac{\\mathbb{P}(A_k|A_{k-1})}{\\mathbb{P}(A_k|A_{k-1},X,L_{1t},L_{2t})}, \\quad t=0,1,2\n\\]\n\n\niptw.mod.stab = glm(A~A_lag1+A_lag2+as.factor(visit),data=dta,family=\"binomial\")\n\npred.iptw.stab = predict(iptw.mod.stab,newdata=dta,type=\"response\")\n\ndta$iptw.stab = dta$A*pred.iptw.stab/pred.iptw+(1-dta$A)*(1-pred.iptw.stab)/(1-pred.iptw)\n\ndta$iptw.stab = ave(dta$iptw.stab,dta$id,FUN=cumprod)\n\nhist(dta$iptw.stab)\n\n\n\n\n\n\n\n\n\nWe will now use the weights to fit an MSM of the form \\(h^{\\underline{a}_0}(t)=h_0(t)e^{\\beta a_t}\\), i.e. an MSM that states that the hazard depends only on current treatment. Try this with the unstabilized and stabilized weights:\n\n\nFit the MSM using a weighted Cox regression. This can be done using coxph} with theweights} option.\nUse the MSM to obtain estimated survival curves under the treatment strategies (i) \\(a_0=a_1=a_2=1\\), (ii) \\(a_0=a_1=a_2=0\\). This can be done using survfit(cox.msm1,newdata=data.frame(A=1)) for strategy (i), for example.\nObtain estimates of the survival probabilities at time 3 under the two treatment strategies, i.e. \\(S^{\\underline{a}_0=1}(3)\\) and \\(S^{\\underline{a}_0=0}(3)\\), and the corresponding risk difference.\n\n\n#---\n#MSM1: Assumes that the hazard depends only on current A\n#change iptw.stab to iptw for unstabilized weights.\ncox.msm1=coxph(Surv(T.start,T.stop,D)~A,\n               data=dta,weights = dta$iptw.stab) \nsummary(cox.msm1)\n\nCall:\ncoxph(formula = Surv(T.start, T.stop, D) ~ A, data = dta, weights = dta$iptw.stab)\n\n  n= 1827, number of events= 187 \n\n     coef exp(coef) se(coef) robust se      z Pr(&gt;|z|)    \nA -0.9920    0.3709   0.1656    0.1894 -5.238 1.63e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n  exp(coef) exp(-coef) lower .95 upper .95\nA    0.3709      2.697    0.2559    0.5375\n\nConcordance= 0.595  (se = 0.018 )\nLikelihood ratio test= 42.55  on 1 df,   p=7e-11\nWald test            = 27.43  on 1 df,   p=2e-07\nScore (logrank) test = 38.89  on 1 df,   p=4e-10,   Robust = 22.6  p=2e-06\n\n  (Note: the likelihood ratio and score tests assume independence of\n     observations within a cluster, the Wald and robust score tests do not).\n\n#---\n#Getting marginal survival curves under the two treatment strategies\nsurv.A1.msm1 = survfit(cox.msm1,newdata=data.frame(A=1))$surv\nsurv.A0.msm1 = survfit(cox.msm1,newdata=data.frame(A=0))$surv\n\n#---\n#Plotting marginal survival curves under the two treatment strategies\ntimes=survfit(cox.msm1,newdata=data.frame(A=0))$time \n\nplot(times,surv.A1.msm1,type=\"s\",col=\"blue\",lwd=2,\n     xlab=\"Time\",ylab=\"Survival probability\",ylim=c(0,1))\nlines(times,surv.A0.msm1,type=\"s\",col=\"red\",lwd=2)\nlegend(\"bottomleft\",c(\"MSM1: Sustained A=1\",\"MSM1: Sustained A=0\"),\n       col=c(\"blue\",\"red\"),lwd=2)\n\n\n\n\n\n\n\n#---\n#survival probabilities and risk difference at time 3\nsummary(survfit(cox.msm1,newdata=data.frame(A=1)),times=3)$surv\n\n[1] 0.7924074\n\nsummary(survfit(cox.msm1,newdata=data.frame(A=0)),times=3)$surv\n\n[1] 0.5339666\n\nrisk.A1.msm1.t3 = 1-summary(survfit(cox.msm1,newdata=data.frame(A=1)),times=3)$surv\nrisk.A0.msm1.t3 = 1-summary(survfit(cox.msm1,newdata=data.frame(A=0)),times=3)$surv\n\nrisk.A1.msm1.t3-risk.A0.msm1.t3\n\n[1] -0.2584408\n\n\n\nRepeat question 3 using an MSM of the following form, where the hazard is allowed to depend on the history of treatment: \\[\nh^{\\underline{a}_0}(t)=h_0(t)e^{\\beta_0 a_t+\\beta_1 a_{t-1}+\\beta_2 a_{t-2}}.\n\\] To obtaining the survival curves under the two treatment strategies based on this MSM, we need to take into account that treatment status \\(A\\) is assumed to be 0 before time, i.e. everyone is untreated before time zero. This means that we cannot obtain the survival probability estimates using a single survfit command (as far as we know!). You may wish to follow the code in the solution for this part.\n\n\n#---\n#MSM2: Assumes that the hazard depends on all lags of A\ncox.msm2=coxph(Surv(T.start,T.stop,D)~A+A_lag1+A_lag2,\n               data=dta,weights = dta$iptw.stab)\nsummary(cox.msm2)\n\nCall:\ncoxph(formula = Surv(T.start, T.stop, D) ~ A + A_lag1 + A_lag2, \n    data = dta, weights = dta$iptw.stab)\n\n  n= 1827, number of events= 187 \n\n          coef exp(coef) se(coef) robust se      z Pr(&gt;|z|)    \nA      -0.8910    0.4103   0.1663    0.1825 -4.882 1.05e-06 ***\nA_lag1 -1.3185    0.2675   0.2998    0.3107 -4.244 2.20e-05 ***\nA_lag2 -0.4622    0.6299   0.4327    0.5288 -0.874    0.382    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n       exp(coef) exp(-coef) lower .95 upper .95\nA         0.4103      2.437    0.2869    0.5867\nA_lag1    0.2675      3.738    0.1455    0.4919\nA_lag2    0.6299      1.588    0.2234    1.7758\n\nConcordance= 0.616  (se = 0.019 )\nLikelihood ratio test= 70.59  on 3 df,   p=3e-15\nWald test            = 38.54  on 3 df,   p=2e-08\nScore (logrank) test = 62.91  on 3 df,   p=1e-13,   Robust = 24.73  p=2e-05\n\n  (Note: the likelihood ratio and score tests assume independence of\n     observations within a cluster, the Wald and robust score tests do not).\n\n#---\n#Getting marginal survival curves under the two treatment strategies\n\n#baseline cumulative hazard\ncumhaz=basehaz(cox.msm2,centered=F)$hazard\ntimes=basehaz(cox.msm2,centered=F)$time \n\n#hazards at each event time, obtained from the increments in the cumulative hazard\nhaz = diff(c(0,cumhaz))\n\n#cumulative hazard and survival probability at each event time \n#under treatment strategy \"always treated\"\n\ncumhaz.A1 = cumsum(haz*exp(cox.msm2$coef[\"A\"]+\n                                  cox.msm2$coef[\"A_lag1\"]*(times&gt;=1)+\n                                  cox.msm2$coef[\"A_lag2\"]*(times&gt;=2)))\ncumhaz.A0 = cumsum(haz)\n\nsurv.A1.msm2 = exp(-cumhaz.A1)\nsurv.A0.msm2 = exp(-cumhaz.A0)\n\n#---\n#Plotting marginal survival curves under the two treatment strategies\nplot(times,surv.A1.msm2,type=\"s\",col=\"blue\",lwd=2,\n     xlab=\"Time\",ylab=\"Survival probability\",ylim=c(0,1))\nlines(times,surv.A0.msm2,type=\"s\",col=\"red\",lwd=2)\nlegend(\"bottomleft\",c(\"MSM2: Sustained A=1\",\"MSM2: Sustained A=0\"),\n       col=c(\"blue\",\"red\"),lwd=2)\n\n\n\n\n\n\n\n#---\n#risk difference at time 3\nrisk.A1.msm2.t3 = 1-stepfun(times,c(1,surv.A1.msm2))(3)\nrisk.A0.msm2.t3 = 1-stepfun(times,c(1,surv.A0.msm2))(3)\n\nrd.msm2.time3 = risk.A1.msm2.t3-risk.A0.msm2.t3\n\n\n#---\n#compare marginal survival curves from the two MSMs\nplot(times,surv.A1.msm1,type=\"s\",col=\"blue\",lwd=2,\n     xlab=\"Time\",ylab=\"Survival probability\",ylim=c(0,1))\nlines(times,surv.A0.msm1,type=\"s\",col=\"red\",lwd=2)\n\nlines(times,surv.A1.msm2,type=\"s\",col=\"blue\",lwd=2,lty=2)\nlines(times,surv.A0.msm2,type=\"s\",col=\"red\",lwd=2,lty=2)\n\nlegend(\"bottomleft\",c(\"MSM1: Sustained A=1\",\"MSM1: Sustained A=0\",\n                      \"MSM2: Sustained A=1\",\"MSM2: Sustained A=0\"),\n       col=rep(c(\"blue\",\"red\"),4),lty=rep(c(1:4),each=2),lwd=2)",
    "crumbs": [
      "Marginal structural models and g-formula for sustained treatment strategies"
    ]
  },
  {
    "objectID": "sustained_trts.html#g-formula",
    "href": "sustained_trts.html#g-formula",
    "title": "Marginal structural models and g-formula for sustained treatment strategies",
    "section": "g-formula",
    "text": "g-formula\n\nFollow the steps below to implement the parametric g-formula `by hand’ to estimate \\(S^{\\underline{a}_0=1}(t)\\) and \\(S^{\\underline{a}_0=0}(t)\\). We hope that this provides some insight into how this method works.\n\nWhat are the estimated marginal survival probabilities at times \\(t=1,2,3\\) under the two treatment strategies?\n\n#---------------------\n#(a) Fit a linear regression for  L1_{k}|L1_{k-1}, L2_{k-1},X,A_{k-1} \n#where k denotes visit, for visits 1 and 2 combined.\n#---------------------\nmod.L1 = lm(L1~A_lag1+X+L1_lag1+L2_lag1+as.factor(T.start),data=dta[dta$T.start&gt;=1,])\nsd.L1 = summary(mod.L1)$sigma#residual SE, used later\n\n#---------------------\n#(b)] Fit a logistic regression for L2_{k}|L1_{k}, L2_{k-1},X,A_{k-1} \n#where k denotes visit, for visits 1 and 2 combined.\n#---------------------\nmod.L2 = glm(L2~A_lag1+X+L1+L2_lag1+as.factor(T.start),\n            data=dta[dta$T.start&gt;=1,],family=\"binomial\")\n\n#---------------------\n#(c) Fit a logistic regression for Y_{k}|L1_{k-1}, L2_{k-1},X,A_{k-1} \n#where k denotes visit, for visits 0, 1, and 2 combined. \n#This is a discrete time hazard model.\n#---------------------\nmod.D = glm(D~A+X+L1+L2+as.factor(T.start),data=dta,family=\"binomial\")\n\n#---------------------\n#(d) Create a new data frame with the same columns names as dta, \n#with 3 rows per individual. This data set will be populated in later steps.\n#---------------------\nn = length(unique(dta$id))\n\ndta.sim = data.frame(id=rep(1:n,each=3),T.start=rep(0:2,n),\n                    X=NA,A=NA,L1=NA,L2=NA,A_lag1=NA,L1_lag1=NA,L2_lag1=NA,haz=NA)\n\n#---------------------\n#(e) In dta.sim set A to a (for a=0,1) at all time points for all individuals. \n#Set X to the observed values.\n#Set L1 and L2 to their observed values at visit 0. \n#---------------------\n#set a to 1 or 0, depending on treatment strategy of interest\na = 0\ndta.sim$A=a\ndta.sim$A_lag1=ifelse(dta.sim$T.start==0,0,a)\n  \ndta.sim$X=rep(dta$X[dta$T.start==0],each=3)\n  \ndta.sim$L1[dta.sim$T.start==0] = dta[dta$T.start==0,]$L1\ndta.sim$L1_lag1[dta.sim$T.start==0] = 0\n  \ndta.sim$L2[dta.sim$T.start==0] = dta[dta$T.start==0,]$L2\ndta.sim$L2_lag1[dta.sim$T.start==0] = 0\n\n#---------------------\n#(f) Simulate a value of L1 at time 1 for each individual by sampling from a normal\n#distribution with mean given by the fitted value from the linear regression in step (a) \n#and standard deviation given by the residual standard error.\n#---------------------\ndta.sim$L1_lag1[dta.sim$T.start==1] = dta.sim$L1[dta.sim$T.start==0]\ndta.sim$L2_lag1[dta.sim$T.start==1] = dta.sim$L2[dta.sim$T.start==0]\nmean.L1 = predict(mod.L1,newdata=dta.sim[dta.sim$T.start==1,],type=\"response\")\ndta.sim$L1[dta.sim$T.start==1] = rnorm(n,mean.L1,sd.L1)\n  \n#---------------------\n#(g) Simulate a value of L2 at time 1 for each individual by sampling from a Bernoulli\n#distribution with probability given by the predicted probabilities from the logistic\n#regression in step (b). \n#---------------------\nmean.L2 = predict(mod.L2,newdata=dta.sim[dta.sim$T.start==1,],type=\"response\")\ndta.sim$L2[dta.sim$T.start==1] = rbinom(n,1,mean.L2)\n  \n#---------------------\n#(h) Simulate a value of L2 at time 2 for each individual in a similar way as in step (f).\n#---------------------\ndta.sim$L1_lag1[dta.sim$T.start==2] = dta.sim$L1[dta.sim$T.start==1]\ndta.sim$L2_lag1[dta.sim$T.start==2] = dta.sim$L2[dta.sim$T.start==1]\nmean.L1 = predict(mod.L1,newdata=dta.sim[dta.sim$T.start==2,],type=\"response\")\ndta.sim$L1[dta.sim$T.start==2] = rnorm(n,mean.L1,sd.L1)\n\n#---------------------\n#(i) Simulate a value of L2 at time 2 for each individual, in a similar way as in step (g).\n#---------------------\nmean.L2 = predict(mod.L2,newdata=dta.sim[dta.sim$T.start==2,],type=\"response\")\ndta.sim$L2[dta.sim$T.start==2] = rbinom(n,1,mean.L2)\n\n#---------------------\n#(j) Estimate the (discrete time) hazard at times t=1,2,3 for each individual \n# using the model fitted in step (c), \n# using the simulated covariate values from previous steps.\n#---------------------\nfor(j in 0:2){\n  dta.sim$haz[dta.sim$T.start==j] = predict(mod.D,newdata=dta.sim[dta.sim$T.start==j,],\n                                           type=\"response\")\n}\n  \n#---------------------\n#(k) Estimate the conditional survival probability under treatment strategy \n# at times t=1,2,3 for each individual \n# using the discrete-time hazards estimated in the previous step.\n#---------------------\ndta.sim$surv.prob = ave(1-dta.sim$haz,dta.sim$id,FUN=cumprod)\n  \n#---------------------\n#(l) Calculate the mean survival probability at times t=1,2,3. \n#This is our estimate of the marginal survival probability \n#under the sustained treatment strategy a0=a1=a2=a\n#---------------------\nsapply(0:2,FUN=function(x){mean(dta.sim$surv.prob[dta.sim$T.start==x])})\n\n[1] 0.8396178 0.6273658 0.4443957\n\n\n\nUse the gformula_survival function in the gfoRmula package to implement the parametric g-formula, by adapting the example in the lecture slides. What are the estimated marginal survival probabilities at times \\(t=1,2,3\\) under the two treatment strategies?\n\n\n#data has to be in the form of a data table\ndta.gform = data.table(dta)\n\n#apply gformula function\ngform =  gformula_survival(obs_data = dta.gform, \n                          id = 'id',\n                          time_points = 3,\n                          time_name = 'T.start', \n                          covnames = c('A','L1','L2'),\n                          covtypes = c('binary','normal','binary'),\n                          covparams = \n                            list(covlink = c('logit', 'identity', 'logit'),\n                                 covmodels = \n                                   c(A ~ lag1_A + X + L1 + L2 + as.factor(T.start),\n                                     L1 ~ lag1_A + X + lag1_L1 + lag1_L2 + as.factor(T.start),\n                                     L2 ~ lag1_A + X + L1 + lag1_L2 + as.factor(T.start))),\n                          histvars = list(c('A', 'L1', 'L2')),\n                          histories = c(lagged),\n                          basecovs = 'X',\n                          outcome_name = 'D',\n                          ymodel = D ~ A + X + L1 + L2 + as.factor(T.start),\n                          intvars = list('A', 'A'),\n                          interventions = list(list(c(static, rep(0, 3))),\n                                               list(c(static, rep(1, 3)))),\n                          int_times = list(c(0:2),c(0:2)),\n                          int_descript = c('Never treat', 'Always treat'),\n                          sim_data_b = FALSE,\n                          seed = 1234,\n                          nsamples = 10, \n                          #number of bootstrap samples: \n                          #set to 10 here in the interests of time, but recommend using 1000\n                          model_fits = TRUE,\n                          show_progress = TRUE)\n\ngform$result\n\n       k Interv.   NP Risk g-form risk     Risk SE Risk lower 95% CI\n   &lt;num&gt;   &lt;num&gt;     &lt;num&gt;       &lt;num&gt;       &lt;num&gt;             &lt;num&gt;\n1:     0       0 0.1080000  0.10800000 0.006600505        0.09890000\n2:     0       1        NA  0.16038223 0.010388395        0.14669448\n3:     0       2        NA  0.05920953 0.009554443        0.04764895\n4:     1       0 0.2003327  0.23334982 0.013176639        0.22456362\n5:     1       1        NA  0.37067984 0.022232153        0.35211392\n6:     1       2        NA  0.10911939 0.016672139        0.08768152\n7:     2       0 0.2646416  0.33492302 0.014608157        0.32086593\n8:     2       1        NA  0.55376512 0.027931134        0.52458603\n9:     2       2        NA  0.14422724 0.021549159        0.11237127\n   Risk upper 95% CI Risk ratio      RR SE RR lower 95% CI RR upper 95% CI\n               &lt;num&gt;      &lt;num&gt;      &lt;num&gt;           &lt;num&gt;           &lt;num&gt;\n1:        0.11855000  1.0000000 0.00000000       1.0000000       1.0000000\n2:        0.17719161  1.4850206 0.08067101       1.3700649       1.6026946\n3:        0.07718767  0.5482364 0.07257958       0.4395280       0.6632936\n4:        0.26157927  1.0000000 0.00000000       1.0000000       1.0000000\n5:        0.41516905  1.5885157 0.07995812       1.4869708       1.7096931\n6:        0.13703452  0.4676215 0.05924834       0.3669187       0.5522950\n7:        0.36597031  1.0000000 0.00000000       1.0000000       1.0000000\n8:        0.60186791  1.6534101 0.08115970       1.5418510       1.7830851\n9:        0.17423417  0.4306280 0.05504768       0.3432467       0.5088472\n   Risk difference       RD SE RD lower 95% CI RD upper 95% CI % Intervened On\n             &lt;num&gt;       &lt;num&gt;           &lt;num&gt;           &lt;num&gt;           &lt;num&gt;\n1:      0.00000000 0.000000000      0.00000000      0.00000000              NA\n2:      0.05238223 0.008233454      0.04187946      0.06554161              NA\n3:     -0.04879047 0.008098353     -0.06101884     -0.03871218              NA\n4:      0.00000000 0.000000000      0.00000000      0.00000000              NA\n5:      0.13733002 0.017580376      0.11825783      0.16928108              NA\n6:     -0.12423043 0.014576530     -0.15336606     -0.10832188              NA\n7:      0.00000000 0.000000000      0.00000000      0.00000000             0.0\n8:      0.21884210 0.024617959      0.18433665      0.25616994            71.0\n9:     -0.19069578 0.017090050     -0.21768185     -0.16683480            77.7\n   Aver % Intervened On\n                  &lt;num&gt;\n1:                   NA\n2:                   NA\n3:                   NA\n4:                   NA\n5:                   NA\n6:                   NA\n7:              0.00000\n8:             39.66667\n9:             43.00000",
    "crumbs": [
      "Marginal structural models and g-formula for sustained treatment strategies"
    ]
  },
  {
    "objectID": "causal_comp_risk.html",
    "href": "causal_comp_risk.html",
    "title": "Estimating causal effects on time-to-event outcomes under competing risks",
    "section": "",
    "text": "Data\nIn this practical we return to the ‘rotterdam’ data set, which includes data on individuals who underwent surgery for primary breast cancer between 1978 and 1993, and whose data were recorded in the Rotterdam Tumour Bank. The data include information on treatments received alongside a number of individual characteristics. Individuals were followed up for disease recurrence and death for up to a maximum of 19.3 years. This data set is available as part of the ‘survival’ package in R, and it has been widely used to illustrate survival analysis methods [e.g. see Royston P, Altman D. External validation of a Cox prognostic model: principles and methods. BMC Medical Research Methodology 2013, 13:33].\nWe will again consider the slightly modified version of the Rotterdam data set in which individual follow-up is recorded in years instead of days, and where we have applied censoring at 10 years. We have also created an additional variable ‘enodes’ which is a transformation of the nodes variable - this transformation has been used in several previous analyses of these data. Some individuals in the original data set have been excluded, as they had recorded death times after they were censored for recurrence, resulting in a final sample size of 2939 individuals.",
    "crumbs": [
      "Estimating causal effects on time-to-event outcomes under competing risks"
    ]
  },
  {
    "objectID": "causal_comp_risk.html#aims",
    "href": "causal_comp_risk.html#aims",
    "title": "Estimating causal effects on time-to-event outcomes under competing risks",
    "section": "Aims",
    "text": "Aims\nThe aim is now to estimate the effect of hormone therapy use on time to the first event to occur, of recurrence and death, up to 10 years. As before, we will estimate the population average (marginal) outcomes we would expect if everyone had received hormone therapy (hormon = 1) and if everyone had not received hormone therapy (hormon = 0) using IPTW weighting and standardisation (g-formula), but this time we will focus on estimation on outcomes under competing risk.",
    "crumbs": [
      "Estimating causal effects on time-to-event outcomes under competing risks"
    ]
  },
  {
    "objectID": "causal_comp_risk.html#load-data-and-packages",
    "href": "causal_comp_risk.html#load-data-and-packages",
    "title": "Estimating causal effects on time-to-event outcomes under competing risks",
    "section": "Load data and packages",
    "text": "Load data and packages\nAs before, load the data and let the treatment variable (hormon) be a factor variable.\nIn this practical we will only use the survival and boot packages.\n\nlibrary(survival)\nlibrary(boot)\n\ndta = readRDS(file = \"data/dta.rds\")\ndta$hormon = as.factor(dta$hormon)",
    "crumbs": [
      "Estimating causal effects on time-to-event outcomes under competing risks"
    ]
  },
  {
    "objectID": "causal_comp_risk.html#simple-analyses-using-a-composite-endpoint",
    "href": "causal_comp_risk.html#simple-analyses-using-a-composite-endpoint",
    "title": "Estimating causal effects on time-to-event outcomes under competing risks",
    "section": "Simple analyses using a composite endpoint",
    "text": "Simple analyses using a composite endpoint\n\nObtain and plot un-adjusted Kaplan-Meier survival curves for time to the composite event of recurrence or death, for people who did and did not receive hormone therapy. How does the result compare to the result you got when looking at time to death only? How do you interpret any differences?\n\n\n# Calculate and plot K-M for time to composite endpoint:\nkmc = survfit(Surv(rdtime, rd != 0) ~ hormon, data=dta)\nplot(kmc, xlab=\"Time (years)\", ylab=\"Survival probability\",\n  col=c(\"red\", \"blue\"), lwd=2, conf.int=T)\nlegend(x=\"bottomleft\", c(\"Treatment: No\", \"Treatment: Yes\"),\n  col=c(\"red\", \"blue\"), lty=1, lwd=2, bty=\"n\")\n# Add grey lines for K-M for time to death only:\nkm = survfit(Surv(dtime, death) ~ hormon, data=dta)\nlines(km, col=c(\"gray\",\"lightgray\"), conf.int=T)\n\n\n\n\n\n\n\n\n\nNow estimate and plot adjusted marginal survival curves for the composite endpoints using the same inverse probability of treatment weights as you made in Exercise 1. Interpret your results.\n\n\n# Fit model for treatment:\nmod.treat = glm(hormon ~ age + meno + size +\n    as.factor(grade) + enodes + pgr + er +\n    chemo, data=dta, family=\"binomial\")\n# Predict the probability of treatment for each individual:\npred.treat = predict(mod.treat, data=dta, type=\"response\")\n# Obtain the weight for each person:\ndta$wt = (dta$hormon==1)/pred.treat + (dta$hormon==0)/(1-pred.treat)\n\n# Calculate and plot weighted K-M for time to composite endpoint:\nkmc.wt = survfit(Surv(rdtime, rd != 0) ~ hormon, weights=wt, data=dta)\nplot(kmc.wt, xlab=\"Time (years)\", ylab=\"Survival probability\",\n  col=c(\"red\", \"blue\"), lwd=2)\nlegend(x=\"bottomleft\", c(\"Treatment: No\", \"Treatment: Yes\"),\n  col=c(\"red\", \"blue\"), lty=1, lwd=2, bty=\"n\")\n\n\n\n\n\n\n\n\n\nRun a test for the difference between composite “survival” curves and interpret the results.\n\n\ncoxph(Surv(rdtime, rd != 0) ~ hormon, weights=wt, data=dta)\n\nCall:\ncoxph(formula = Surv(rdtime, rd != 0) ~ hormon, data = dta, weights = wt)\n\n           coef exp(coef) se(coef) robust se      z     p\nhormon1 -0.2258    0.7979   0.0362    0.1437 -1.571 0.116\n\nLikelihood ratio test=39.05  on 1 df, p=4.137e-10\nn= 2939, number of events= 1612",
    "crumbs": [
      "Estimating causal effects on time-to-event outcomes under competing risks"
    ]
  },
  {
    "objectID": "causal_comp_risk.html#estimating-cause-specific-cumulative-incidence-using-ipw",
    "href": "causal_comp_risk.html#estimating-cause-specific-cumulative-incidence-using-ipw",
    "title": "Estimating causal effects on time-to-event outcomes under competing risks",
    "section": "Estimating cause-specific cumulative incidence using IPW",
    "text": "Estimating cause-specific cumulative incidence using IPW\nFrom now on, say that the main event of interest is recurrence, with the competing event of death (without recurrence) being present.\n\nCalculate and plot unadjusted marginal cause-specific cumulative incidence for both recurrence and death without recurrence. How do these curves compare to the plot in Part A 1?\n\n\ncuminc = survfit(Surv(rdtime, rd, type=\"mstate\") ~ hormon, dta)\nplot(cuminc, xlab=\"Time (years)\", ylab=\"Cumulative incidence\",\n  col=c(\"red\", \"blue\"), lwd=2, lty=c(3, 3, 1, 1))\nlegend(x=\"topleft\", c(\"Recurrence (treated)\", \"Recurrence (untreated)\",\n  \"Death (treated)\", \"Death (untreated)\"),\n  col=c(\"blue\", \"red\", \"blue\", \"red\"), lty=c(3, 3, 1, 1),\n  lwd=2, bty=\"n\")\n\n\n\n\n\n\n\n\n\nCalculate and plot adjusted marginal cause-specific cumulative incidence for both recurrence and death without recurrence. How would you describe the total effect of treatment on recurrence? To what degree can there be a indirect effect through the competing event of death without recurrence?\n\n\ncuminc = survfit(Surv(rdtime, rd, type=\"mstate\") ~ hormon, weights=wt, dta)\nplot(cuminc, xlab=\"Time (years)\", ylab=\"Cumulative incidence\",\n  col=c(\"red\", \"blue\"), lwd=2, lty=c(3, 3, 1, 1))\nlegend(x=\"topleft\", c(\"Recurrence (untreated)\", \"Recurrence (treated)\",\n  \"Death (untreated)\", \"Death (treated)\"),\n  col=c(\"red\", \"blue\", \"red\", \"blue\"), lty=c(3, 3, 1, 1),\n  lwd=2, bty=\"n\")\n\n\n\n\n\n\n\n\n\nRun a test for the difference between cumulative incidence of recurrence and interpret the results.\n\n\n# Create new dataset for the subdistribution hazard:\ndta.sub = finegray(Surv(rdtime, rd, type=\"mstate\") ~ ., etype=1, data=dta)\n# Run weighted Cox model on subdistribution dataset:\ncoxph(Surv(fgstart, fgstop, fgstatus) ~ hormon, id=pid,\n  weight=wt*fgwt, robust=T, data=dta.sub)\n\nCall:\ncoxph(formula = Surv(fgstart, fgstop, fgstatus) ~ hormon, data = dta.sub, \n    weights = wt * fgwt, robust = T, id = pid)\n\n            coef exp(coef) se(coef) robust se      z     p\nhormon1 -0.17199   0.84199  0.03747   0.14621 -1.176 0.239\n\nLikelihood ratio test=21.12  on 1 df, p=4.315e-06\nn= 29673, unique id= 2939, number of events= 1477 \n\n\n\nCalculate the absolute difference in five year risk of recurrence between the two treatment groups based on the weighted Kaplan-Meier estimates from the prior exercise. Add bootstrap confidence intervals. How do you interpret the results?\n\n\n# Calculate difference in cumulative incidence F(t) at t=5 years:\npstate = summary(cuminc, times=5)$pstate\nFdiff = pstate[2,2] - pstate[1,2]\nFdiff\n\n          1 \n-0.09252237 \n\n# Bootstrap:\nfdiff = function(data, indices){\n  dataset = data[indices,]\n  # Recalculate weights for each bootstrap sample:\n  mod.treat = glm(hormon ~ age + meno + size +\n      as.factor(grade) + enodes + pgr + er +\n      chemo, data=dataset, family=\"binomial\")\n  pred.treat = predict(mod.treat, data=dataset, type=\"response\")\n  data$wt = (dataset$hormon==1)/pred.treat +\n    (dataset$hormon==0)/(1-pred.treat)\n  # Calculate weighed difference in cumulative incidence:\n  cuminc = survfit(Surv(rdtime, rd, type=\"mstate\") ~ hormon, weights=wt, dataset)\n  pstate = summary(cuminc, times=5)$pstate\n  return(pstate[2,2] - pstate[1,2])\n}\nb1 = boot(data=dta, statistic=fdiff, R=100)\nb1\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = dta, statistic = fdiff, R = 100)\n\n\nBootstrap Statistics :\n       original       bias    std. error\nt1* -0.09252237 -0.002111384  0.04551915\n\nboot.ci(b1, type=c(\"perc\", \"norm\"))\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 100 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = b1, type = c(\"perc\", \"norm\"))\n\nIntervals : \nLevel      Normal             Percentile     \n95%   (-0.1796, -0.0012 )   (-0.1906, -0.0025 )  \nCalculations and Intervals on Original Scale\nSome percentile intervals may be unstable\n\n\n\nRepeat exercise 3, but now for the restricted mean (recurrence free) time lost (RMTL) to recurrence after five years. Interpret the results.\n\n\n# Calculate difference in RMTL at 5 years:\nrmtl = summary(cuminc, rmean=5)$table\nrmtl\n\n                  n     nevent      rmean  se(rmean)\nhormon=0, (s0) 2605    0.00000 3.69900478 0.03463876\nhormon=1, (s0)  334    0.00000 4.11459536 0.11769994\nhormon=0, 1    2605 1524.52988 1.22911433 0.03444235\nhormon=1, 1     334 1342.60996 0.84317703 0.11566147\nhormon=0, 2    2605  142.32192 0.07188089 0.01023798\nhormon=1, 2     334   69.73814 0.04222762 0.01360438\n\nRMTLdiff = rmtl[4, 3] - rmtl[3, 3]\nRMTLdiff\n\n[1] -0.3859373\n\n# Bootstrap:\nfdiff = function(data, indices){\n  dataset = data[indices,]\n  # Recalculate weights for each bootstrap sample:\n  mod.treat = glm(hormon ~ age + meno + size +\n      as.factor(grade) + enodes + pgr + er +\n      chemo, data=dataset, family=\"binomial\")\n  pred.treat = predict(mod.treat, data=dataset, type=\"response\")\n  data$wt = (dataset$hormon==1)/pred.treat +\n    (dataset$hormon==0)/(1-pred.treat)\n  # Calculate weighed difference in cumulative incidence:\n  cuminc = survfit(Surv(rdtime, rd, type=\"mstate\") ~ hormon, weights=wt, dataset)\n  rmtl = summary(cuminc, rmean=5)$table\n  return(rmtl[4, 3] - rmtl[3, 3])\n}\nb2 = boot(data=dta, statistic=fdiff, R=100)\nb2\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = dta, statistic = fdiff, R = 100)\n\n\nBootstrap Statistics :\n      original      bias    std. error\nt1* -0.3859373 0.001982641   0.1115977\n\nboot.ci(b2, type=c(\"perc\", \"norm\"))\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 100 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = b2, type = c(\"perc\", \"norm\"))\n\nIntervals : \nLevel      Normal             Percentile     \n95%   (-0.6066, -0.1692 )   (-0.6164, -0.1376 )  \nCalculations and Intervals on Original Scale\nSome percentile intervals may be unstable\n\n\n\nEXTRA: Make a plot of the difference between the cumulative incidence curves from \\(t=0\\) to \\(t=10\\) and interpret the results (hint: utilize the stepfun function)\n\n\nFa1 = stepfun(cuminc[\"hormon=1\",]$time, c(0, cuminc[\"hormon=1\",]$pstate[,2]), right=T)\nFa0 = stepfun(cuminc[\"hormon=0\",]$time, c(0, cuminc[\"hormon=0\",]$pstate[,2]), right=T)\ntimes = seq(0, 10, by=0.01)\nplot(times, Fa1(times) - Fa0(times), type=\"s\", lty=3, lwd=2,\n  ylab=\"Cumulative incidence difference\", xlab=\"Time (years)\")",
    "crumbs": [
      "Estimating causal effects on time-to-event outcomes under competing risks"
    ]
  },
  {
    "objectID": "causal_comp_risk.html#estimating-cause-specific-cumulative-incidence-using-standardisation-g-formula",
    "href": "causal_comp_risk.html#estimating-cause-specific-cumulative-incidence-using-standardisation-g-formula",
    "title": "Estimating causal effects on time-to-event outcomes under competing risks",
    "section": "Estimating cause-specific cumulative incidence using standardisation (g-formula)",
    "text": "Estimating cause-specific cumulative incidence using standardisation (g-formula)\n\nFit a Cox proportional hazard model for the cause specific hazard of each event (recurrence and death without recurrence), adjusting for hormon, age, meno, size, grade, enodes, pgr, er and chemo. How would you interpret the output?\n\n\nc1 = coxph(Surv(rdtime, rd, type=\"mstate\") ~ hormon + age +\n    meno + size + as.factor(grade) + enodes + pgr +\n    er + chemo, id=pid, dta)\nc1\n\nCall:\ncoxph(formula = Surv(rdtime, rd, type = \"mstate\") ~ hormon + \n    age + meno + size + as.factor(grade) + enodes + pgr + er + \n    chemo, data = dta, id = pid)\n\n                   \n1:2                       coef  exp(coef)   se(coef)  robust se       z\n  hormon1           -3.016e-01  7.396e-01  8.503e-02  8.880e-02  -3.397\n  age               -1.531e-02  9.848e-01  3.572e-03  3.871e-03  -3.954\n  meno               1.579e-01  1.171e+00  9.071e-02  9.679e-02   1.632\n  size20-50          3.114e-01  1.365e+00  5.964e-02  5.975e-02   5.211\n  size&gt;50            4.991e-01  1.647e+00  9.064e-02  9.883e-02   5.050\n  as.factor(grade)3  3.454e-01  1.413e+00  6.611e-02  6.478e-02   5.332\n  enodes            -1.929e+00  1.453e-01  1.009e-01  1.185e-01 -16.279\n  pgr               -9.318e-05  9.999e-01  1.073e-04  1.097e-04  -0.849\n  er                -6.956e-05  9.999e-01  1.061e-04  1.119e-04  -0.622\n  chemo             -3.134e-01  7.309e-01  7.336e-02  7.655e-02  -4.095\n                   \n1:2                        p\n  hormon1           0.000682\n  age               7.67e-05\n  meno              0.102761\n  size20-50         1.87e-07\n  size&gt;50           4.41e-07\n  as.factor(grade)3 9.69e-08\n  enodes             &lt; 2e-16\n  pgr               0.395650\n  er                0.534111\n  chemo             4.23e-05\n\n                   \n1:3                       coef  exp(coef)   se(coef)  robust se      z       p\n  hormon1           -0.3382738  0.7130000  0.2657453  0.2598436 -1.302 0.19297\n  age                0.1346077  1.1440878  0.0122801  0.0124025 10.853 &lt; 2e-16\n  meno              -0.5160600  0.5968676  0.4601891  0.4658301 -1.108 0.26794\n  size20-50          0.1388994  1.1490085  0.2007498  0.2038521  0.681 0.49564\n  size&gt;50            0.2992439  1.3488385  0.2873388  0.2851762  1.049 0.29403\n  as.factor(grade)3 -0.0180504  0.9821115  0.1990415  0.1960833 -0.092 0.92665\n  enodes            -1.1990698  0.3014745  0.3621725  0.3346957 -3.583 0.00034\n  pgr                0.0001656  1.0001656  0.0003013  0.0002582  0.641 0.52133\n  er                -0.0002871  0.9997130  0.0003064  0.0003764 -0.763 0.44563\n  chemo              0.0429707  1.0439073  0.4871352  0.4957296  0.087 0.93092\n\n States:  1= (s0), 2= 1, 3= 2 \n\nLikelihood ratio test=851.2  on 20 df, p=&lt; 2.2e-16\nn= 2939, unique id= 2939, number of events= 1612 \n\n\n\nUse the Cox model you just fitted to predict the event probabilities from \\(t=0\\) to \\(t=10\\) for an individual with hormon, age, size and grade at the same values as the first individual in the dataset (pid=1) and all other covariates equal to 0.\n\n\nnd = expand.grid(hormon = dta$hormon[1], age = dta$age[1], meno = c(0),\n  size = dta$size[1], grade = dta$grade[1], enodes = c(0),\n  pgr = c(0), er = c(0), chemo = c(0))\ncuminc.pred = survfit(c1, newdata=nd)\nplot(cuminc.pred, lwd=c(2, 2), lty=c(3, 1), ylab=\"Cumulative incidence\", xlab=\"Time\")\nlegend(x=\"topleft\", c(\"Recurrence\", \"Death\"), lty=c(3, 1), lwd=c(2, 2), bty=\"n\")\n\n\n\n\n\n\n\n\n\nCalculate marginal adjusted cumulative incidence curves for recurrence by standardisation (g-formula). How do the resulting curves compare to the inverse probability weighted curves produced earlier?\n\nHint: Predict cumulative incidence as in the previous exercise, for every individual in the dataset, fixing treatment to 1 and then, again, to 0.\n\n# Predict cumulative incidence for hormon=1 and hormon=0, with observed values of L:\nnd1 = dta\nnd0 = dta\nnd1$hormon = \"1\"; nd0$hormon = \"0\"\ncuminc.1 = survfit(c1, newdata=nd1)\ncuminc.0 = survfit(c1, newdata=nd0)\n# Average over individual predictions and plot:\nplot(cuminc.0$time, rowMeans(cuminc.0$pstate[,,2]), type=\"s\", col=\"red\", lwd=2, lty=3,\n  xlab=\"Time\", ylab=\"Cumulative incidence\")\nlines(cuminc.1$time, rowMeans(cuminc.1$pstate[,,2]), type=\"s\", col=\"blue\", lwd=2, lty=3)\nlegend(x=\"topleft\", c(\"Recurrence (untreated)\", \"Recurrence (treated)\"),\n  col=c(\"red\", \"blue\"), lwd=2, lty=3, bty=\"n\")",
    "crumbs": [
      "Estimating causal effects on time-to-event outcomes under competing risks"
    ]
  }
]